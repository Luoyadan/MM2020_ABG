Train: [1][0/12], lr: 0.03000	Time 10.858 (10.858)	Data 6.817 (6.817)	Prec@1 11.719 (11.719)	Prec@5 40.625 (40.625)	Loss 5.2574 (5.2574)   loss_c 2.4848	beta 0.750, 0.750, 0.500  loss_a 2.7726	
Train: [2][0/12], lr: 0.03000	Time 6.875 (6.875)	Data 6.799 (6.799)	Prec@1 17.969 (17.969)	Prec@5 52.344 (52.344)	Loss 5.1639 (5.1639)   loss_c 2.3892	beta 0.750, 0.750, 0.500  loss_a 2.7747	
Train: [3][0/12], lr: 0.03000	Time 6.784 (6.784)	Data 6.699 (6.699)	Prec@1 17.188 (17.188)	Prec@5 72.656 (72.656)	Loss 4.9463 (4.9463)   loss_c 2.1655	beta 0.750, 0.750, 0.500  loss_a 2.7808	
Train: [4][0/12], lr: 0.03000	Time 6.148 (6.148)	Data 6.065 (6.065)	Prec@1 55.469 (55.469)	Prec@5 82.812 (82.812)	Loss 4.3569 (4.3569)   loss_c 1.5806	beta 0.750, 0.750, 0.500  loss_a 2.7763	
Train: [5][0/12], lr: 0.03000	Time 6.235 (6.235)	Data 6.156 (6.156)	Prec@1 63.281 (63.281)	Prec@5 93.750 (93.750)	Loss 3.8340 (3.8340)   loss_c 1.0669	beta 0.750, 0.750, 0.500  loss_a 2.7671	
Train: [6][0/12], lr: 0.03000	Time 6.724 (6.724)	Data 6.675 (6.675)	Prec@1 78.125 (78.125)	Prec@5 99.219 (99.219)	Loss 3.4594 (3.4594)   loss_c 0.6889	beta 0.750, 0.750, 0.500  loss_a 2.7705	
Train: [7][0/12], lr: 0.03000	Time 6.373 (6.373)	Data 6.310 (6.310)	Prec@1 82.031 (82.031)	Prec@5 99.219 (99.219)	Loss 3.3666 (3.3666)   loss_c 0.5199	beta 0.750, 0.750, 0.500  loss_a 2.8467	
Train: [8][0/12], lr: 0.03000	Time 6.432 (6.432)	Data 6.361 (6.361)	Prec@1 86.719 (86.719)	Prec@5 99.219 (99.219)	Loss 3.1865 (3.1865)   loss_c 0.4438	beta 0.750, 0.750, 0.500  loss_a 2.7427	
Train: [9][0/12], lr: 0.03000	Time 6.392 (6.392)	Data 6.326 (6.326)	Prec@1 92.969 (92.969)	Prec@5 99.219 (99.219)	Loss 2.9736 (2.9736)   loss_c 0.3222	beta 0.750, 0.750, 0.500  loss_a 2.6514	
Train: [10][0/12], lr: 0.00300	Time 6.141 (6.141)	Data 6.069 (6.069)	Prec@1 91.406 (91.406)	Prec@5 100.000 (100.000)	Loss 3.0014 (3.0014)   loss_c 0.2272	beta 0.750, 0.750, 0.500  loss_a 2.7742	
Train: [11][0/12], lr: 0.00300	Time 6.385 (6.385)	Data 6.310 (6.310)	Prec@1 97.656 (97.656)	Prec@5 100.000 (100.000)	Loss 2.7304 (2.7304)   loss_c 0.1187	beta 0.750, 0.750, 0.500  loss_a 2.6116	
Train: [12][0/12], lr: 0.00300	Time 6.490 (6.490)	Data 6.418 (6.418)	Prec@1 97.656 (97.656)	Prec@5 100.000 (100.000)	Loss 2.7739 (2.7739)   loss_c 0.1324	beta 0.750, 0.750, 0.500  loss_a 2.6415	
Train: [13][0/12], lr: 0.00300	Time 6.361 (6.361)	Data 6.302 (6.302)	Prec@1 93.750 (93.750)	Prec@5 100.000 (100.000)	Loss 2.8561 (2.8561)   loss_c 0.1838	beta 0.750, 0.750, 0.500  loss_a 2.6723	
Train: [14][0/12], lr: 0.00300	Time 6.550 (6.550)	Data 6.465 (6.465)	Prec@1 95.312 (95.312)	Prec@5 100.000 (100.000)	Loss 2.8406 (2.8406)   loss_c 0.1727	beta 0.750, 0.750, 0.500  loss_a 2.6679	
Train: [15][0/12], lr: 0.00300	Time 6.761 (6.761)	Data 6.683 (6.683)	Prec@1 93.750 (93.750)	Prec@5 100.000 (100.000)	Loss 2.8049 (2.8049)   loss_c 0.1380	beta 0.750, 0.750, 0.500  loss_a 2.6668	
Train: [16][0/12], lr: 0.00300	Time 6.547 (6.547)	Data 6.475 (6.475)	Prec@1 95.312 (95.312)	Prec@5 100.000 (100.000)	Loss 2.7813 (2.7813)   loss_c 0.1295	beta 0.750, 0.750, 0.500  loss_a 2.6518	
  loss_c 0.1093	beta 0.750, 0.750, 0.500  loss_a 2.7305	loss_edge 0.0000	
