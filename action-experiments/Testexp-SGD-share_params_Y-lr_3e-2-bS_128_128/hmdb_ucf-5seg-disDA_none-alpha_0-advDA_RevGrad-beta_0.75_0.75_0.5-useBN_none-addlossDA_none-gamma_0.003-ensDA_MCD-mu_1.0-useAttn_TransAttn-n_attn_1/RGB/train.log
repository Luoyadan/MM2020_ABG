Train: [1][0/12], lr: 0.03000	Time 9.199 (9.199)	Data 5.227 (5.227)	Prec@1 5.469 (5.469)	Prec@5 53.906 (53.906)	Loss 8.6477 (8.6477)   loss_c 4.9710	beta 0.750, 0.750, 0.500  loss_a 2.7725	mu 1.000000  loss_s -0.0021	loss_edge 0.9063	
Train: [2][0/12], lr: 0.03000	Time 5.797 (5.797)	Data 5.614 (5.614)	Prec@1 28.125 (28.125)	Prec@5 60.938 (60.938)	Loss 7.0775 (7.0775)   loss_c 4.3132	beta 0.750, 0.750, 0.500  loss_a 2.7748	mu 1.000000  loss_s -0.0122	loss_edge 0.0018	
Train: [3][0/12], lr: 0.03000	Time 6.384 (6.384)	Data 6.247 (6.247)	Prec@1 49.219 (49.219)	Prec@5 84.375 (84.375)	Loss 5.9754 (5.9754)   loss_c 3.2044	beta 0.750, 0.750, 0.500  loss_a 2.7800	mu 1.000000  loss_s -0.0121	loss_edge 0.0031	
Train: [4][0/12], lr: 0.03000	Time 5.797 (5.797)	Data 5.645 (5.645)	Prec@1 59.375 (59.375)	Prec@5 94.531 (94.531)	Loss 5.1246 (5.1246)   loss_c 2.3543	beta 0.750, 0.750, 0.500  loss_a 2.7763	mu 1.000000  loss_s -0.0102	loss_edge 0.0043	
Train: [5][0/12], lr: 0.03000	Time 5.422 (5.422)	Data 5.271 (5.271)	Prec@1 71.094 (71.094)	Prec@5 93.750 (93.750)	Loss 4.6120 (4.6120)   loss_c 1.8447	beta 0.750, 0.750, 0.500  loss_a 2.7717	mu 1.000000  loss_s -0.0113	loss_edge 0.0069	
Train: [6][0/12], lr: 0.03000	Time 5.656 (5.656)	Data 5.496 (5.496)	Prec@1 78.125 (78.125)	Prec@5 96.875 (96.875)	Loss 4.1141 (4.1141)   loss_c 1.3591	beta 0.750, 0.750, 0.500  loss_a 2.7619	mu 1.000000  loss_s -0.0110	loss_edge 0.0042	
Train: [7][0/12], lr: 0.03000	Time 5.933 (5.933)	Data 5.775 (5.775)	Prec@1 75.781 (75.781)	Prec@5 97.656 (97.656)	Loss 4.2593 (4.2593)   loss_c 1.4995	beta 0.750, 0.750, 0.500  loss_a 2.7647	mu 1.000000  loss_s -0.0104	loss_edge 0.0054	
Train: [8][0/12], lr: 0.03000	Time 5.566 (5.566)	Data 5.439 (5.439)	Prec@1 82.031 (82.031)	Prec@5 98.438 (98.438)	Loss 3.9248 (3.9248)   loss_c 1.1055	beta 0.750, 0.750, 0.500  loss_a 2.8245	mu 1.000000  loss_s -0.0109	loss_edge 0.0057	
Train: [9][0/12], lr: 0.03000	Time 5.603 (5.603)	Data 5.443 (5.443)	Prec@1 88.281 (88.281)	Prec@5 100.000 (100.000)	Loss 3.5220 (3.5220)   loss_c 0.6986	beta 0.750, 0.750, 0.500  loss_a 2.8257	mu 1.000000  loss_s -0.0081	loss_edge 0.0058	
Train: [10][0/12], lr: 0.00300	Time 5.613 (5.613)	Data 5.469 (5.469)	Prec@1 86.719 (86.719)	Prec@5 100.000 (100.000)	Loss 3.6326 (3.6326)   loss_c 0.8734	beta 0.750, 0.750, 0.500  loss_a 2.7595	mu 1.000000  loss_s -0.0079	loss_edge 0.0075	
Train: [11][0/12], lr: 0.00300	Time 5.642 (5.642)	Data 5.516 (5.516)	Prec@1 86.719 (86.719)	Prec@5 99.219 (99.219)	Loss 3.3847 (3.3847)   loss_c 0.6559	beta 0.750, 0.750, 0.500  loss_a 2.7276	mu 1.000000  loss_s -0.0096	loss_edge 0.0108	
Train: [12][0/12], lr: 0.00300	Time 5.893 (5.893)	Data 5.710 (5.710)	Prec@1 91.406 (91.406)	Prec@5 97.656 (97.656)	Loss 3.3095 (3.3095)   loss_c 0.6379	beta 0.750, 0.750, 0.500  loss_a 2.6732	mu 1.000000  loss_s -0.0076	loss_edge 0.0060	
Train: [13][0/12], lr: 0.00300	Time 5.407 (5.407)	Data 5.246 (5.246)	Prec@1 89.844 (89.844)	Prec@5 100.000 (100.000)	Loss 3.2260 (3.2260)   loss_c 0.4957	beta 0.750, 0.750, 0.500  loss_a 2.7304	mu 1.000000  loss_s -0.0101	loss_edge 0.0099	
Train: [14][0/12], lr: 0.00300	Time 5.945 (5.945)	Data 5.774 (5.774)	Prec@1 95.312 (95.312)	Prec@5 100.000 (100.000)	Loss 3.0152 (3.0152)   loss_c 0.3466	beta 0.750, 0.750, 0.500  loss_a 2.6728	mu 1.000000  loss_s -0.0097	loss_edge 0.0055	
Train: [15][0/12], lr: 0.00300	Time 6.080 (6.080)	Data 5.918 (5.918)	Prec@1 92.188 (92.188)	Prec@5 98.438 (98.438)	Loss 3.1534 (3.1534)   loss_c 0.5033	beta 0.750, 0.750, 0.500  loss_a 2.6518	mu 1.000000  loss_s -0.0095	loss_edge 0.0078	
Train: [16][0/12], lr: 0.00300	Time 5.558 (5.558)	Data 5.376 (5.376)	Prec@1 88.281 (88.281)	Prec@5 100.000 (100.000)	Loss 3.3425 (3.3425)   loss_c 0.6046	beta 0.750, 0.750, 0.500  loss_a 2.7406	mu 1.000000  loss_s -0.0085	loss_edge 0.0059	
Train: [17][0/12], lr: 0.00300	Time 5.773 (5.773)	Data 5.627 (5.627)	Prec@1 94.531 (94.531)	Prec@5 99.219 (99.219)	Loss 3.1365 (3.1365)   loss_c 0.4398	beta 0.750, 0.750, 0.500  loss_a 2.7013	mu 1.000000  loss_s -0.0098	loss_edge 0.0051	
