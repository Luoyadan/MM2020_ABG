Train: [1][0/12], lr: 0.03000	Time 9.628 (9.628)	Data 5.077 (5.077)	Prec@1 7.812 (7.812)	Prec@5 49.219 (49.219)	Loss 8.3579 (8.3579)   loss_c 4.9688	beta 0.750, 0.750, 0.500  loss_a 2.7726	mu 4.000000  loss_s -0.0023	loss_edge 0.6187	
Train: [2][0/12], lr: 0.03000	Time 5.966 (5.966)	Data 5.520 (5.520)	Prec@1 45.312 (45.312)	Prec@5 78.125 (78.125)	Loss 6.2178 (6.2178)   loss_c 3.4542	beta 0.750, 0.750, 0.500  loss_a 2.7748	mu 4.000000  loss_s -0.0111	loss_edge 0.0000	
Train: [3][0/12], lr: 0.03000	Time 5.972 (5.972)	Data 5.515 (5.515)	Prec@1 84.375 (84.375)	Prec@5 98.438 (98.438)	Loss 4.0195 (4.0195)   loss_c 1.2520	beta 0.750, 0.750, 0.500  loss_a 2.7803	mu 4.000000  loss_s -0.0128	loss_edge 0.0000	
Train: [4][0/12], lr: 0.03000	Time 5.825 (5.825)	Data 5.351 (5.351)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)	Loss 3.3197 (3.3197)   loss_c 0.5573	beta 0.750, 0.750, 0.500  loss_a 2.7721	mu 4.000000  loss_s -0.0097	loss_edge 0.0000	
Train: [5][0/12], lr: 0.03000	Time 6.060 (6.060)	Data 5.617 (5.617)	Prec@1 96.875 (96.875)	Prec@5 100.000 (100.000)	Loss 3.0007 (3.0007)   loss_c 0.2532	beta 0.750, 0.750, 0.500  loss_a 2.7609	mu 4.000000  loss_s -0.0135	loss_edge 0.0000	
Train: [6][0/12], lr: 0.03000	Time 5.865 (5.865)	Data 5.410 (5.410)	Prec@1 96.875 (96.875)	Prec@5 100.000 (100.000)	Loss 2.9794 (2.9794)   loss_c 0.2427	beta 0.750, 0.750, 0.500  loss_a 2.7511	mu 4.000000  loss_s -0.0145	loss_edge 0.0000	
Train: [7][0/12], lr: 0.03000	Time 5.768 (5.768)	Data 5.328 (5.328)	Prec@1 96.094 (96.094)	Prec@5 100.000 (100.000)	Loss 2.9974 (2.9974)   loss_c 0.1745	beta 0.750, 0.750, 0.500  loss_a 2.8331	mu 4.000000  loss_s -0.0103	loss_edge 0.0000	
Train: [8][0/12], lr: 0.03000	Time 5.815 (5.815)	Data 5.294 (5.294)	Prec@1 97.656 (97.656)	Prec@5 100.000 (100.000)	Loss 3.1453 (3.1453)   loss_c 0.1756	beta 0.750, 0.750, 0.500  loss_a 2.9824	mu 4.000000  loss_s -0.0127	loss_edge 0.0000	
Train: [9][0/12], lr: 0.03000	Time 5.935 (5.935)	Data 5.509 (5.509)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 2.8585 (2.8585)   loss_c 0.1081	beta 0.750, 0.750, 0.500  loss_a 2.7606	mu 4.000000  loss_s -0.0101	loss_edge 0.0001	
Train: [10][0/12], lr: 0.00300	Time 5.779 (5.779)	Data 5.319 (5.319)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.8209 (2.8209)   loss_c 0.0719	beta 0.750, 0.750, 0.500  loss_a 2.7614	mu 4.000000  loss_s -0.0124	loss_edge 0.0001	
Train: [11][0/12], lr: 0.00300	Time 5.926 (5.926)	Data 5.478 (5.478)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 2.7308 (2.7308)   loss_c 0.0598	beta 0.750, 0.750, 0.500  loss_a 2.6812	mu 4.000000  loss_s -0.0103	loss_edge 0.0000	
Train: [12][0/12], lr: 0.00300	Time 5.783 (5.783)	Data 5.331 (5.331)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.6029 (2.6029)   loss_c 0.0315	beta 0.750, 0.750, 0.500  loss_a 2.5799	mu 4.000000  loss_s -0.0085	loss_edge 0.0001	
Train: [13][0/12], lr: 0.00300	Time 5.978 (5.978)	Data 5.544 (5.544)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.7544 (2.7544)   loss_c 0.0358	beta 0.750, 0.750, 0.500  loss_a 2.7274	mu 4.000000  loss_s -0.0089	loss_edge 0.0000	
Train: [14][0/12], lr: 0.00300	Time 6.122 (6.122)	Data 5.670 (5.670)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 2.6870 (2.6870)   loss_c 0.0516	beta 0.750, 0.750, 0.500  loss_a 2.6428	mu 4.000000  loss_s -0.0074	loss_edge 0.0000	
Train: [15][0/12], lr: 0.00300	Time 5.968 (5.968)	Data 5.552 (5.552)	Prec@1 98.438 (98.438)	Prec@5 100.000 (100.000)	Loss 2.7334 (2.7334)   loss_c 0.0734	beta 0.750, 0.750, 0.500  loss_a 2.6691	mu 4.000000  loss_s -0.0092	loss_edge 0.0000	
Train: [16][0/12], lr: 0.00300	Time 6.071 (6.071)	Data 5.572 (5.572)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.6944 (2.6944)   loss_c 0.0267	beta 0.750, 0.750, 0.500  loss_a 2.6756	mu 4.000000  loss_s -0.0079	loss_edge 0.0000	
Train: [17][0/12], lr: 0.00300	Time 5.555 (5.555)	Data 5.096 (5.096)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.7208 (2.7208)   loss_c 0.0369	beta 0.750, 0.750, 0.500  loss_a 2.6952	mu 4.000000  loss_s -0.0113	loss_edge 0.0001	
Train: [18][0/12], lr: 0.00300	Time 5.820 (5.820)	Data 5.371 (5.371)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 2.7925 (2.7925)   loss_c 0.0781	beta 0.750, 0.750, 0.500  loss_a 2.7227	mu 4.000000  loss_s -0.0083	loss_edge 0.0000	
Train: [19][0/12], lr: 0.00300	Time 5.941 (5.941)	Data 5.502 (5.502)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.7617 (2.7617)   loss_c 0.0410	beta 0.750, 0.750, 0.500  loss_a 2.7277	mu 4.000000  loss_s -0.0070	loss_edge 0.0000	
Train: [20][0/12], lr: 0.00030	Time 5.758 (5.758)	Data 5.293 (5.293)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 2.8060 (2.8060)   loss_c 0.0681	beta 0.750, 0.750, 0.500  loss_a 2.7455	mu 4.000000  loss_s -0.0076	loss_edge 0.0000	
Train: [21][0/12], lr: 0.00030	Time 6.357 (6.357)	Data 5.872 (5.872)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.7282 (2.7282)   loss_c 0.0218	beta 0.750, 0.750, 0.500  loss_a 2.7152	mu 4.000000  loss_s -0.0089	loss_edge 0.0000	
Train: [22][0/12], lr: 0.00030	Time 5.842 (5.842)	Data 5.421 (5.421)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 2.7171 (2.7171)   loss_c 0.0467	beta 0.750, 0.750, 0.500  loss_a 2.6776	mu 4.000000  loss_s -0.0072	loss_edge 0.0000	
Train: [23][0/12], lr: 0.00030	Time 6.140 (6.140)	Data 5.655 (5.655)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.7629 (2.7629)   loss_c 0.0374	beta 0.750, 0.750, 0.500  loss_a 2.7325	mu 4.000000  loss_s -0.0071	loss_edge 0.0001	
Train: [24][0/12], lr: 0.00030	Time 5.975 (5.975)	Data 5.540 (5.540)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 2.8884 (2.8884)   loss_c 0.1075	beta 0.750, 0.750, 0.500  loss_a 2.7908	mu 4.000000  loss_s -0.0100	loss_edge 0.0000	
