Train: [1][0/12], lr: 0.03000	Time 10.193 (10.193)	Data 5.414 (5.414)	Prec@1 7.812 (7.812)	Prec@5 49.219 (49.219)	Loss 8.3579 (8.3579)   loss_c 4.9688	beta 0.750, 0.750, 0.500  loss_a 2.7726	mu 4.000000  loss_s -0.0023	loss_edge 0.6187	
Train: [2][0/12], lr: 0.03000	Time 5.741 (5.741)	Data 5.269 (5.269)	Prec@1 45.312 (45.312)	Prec@5 77.344 (77.344)	Loss 6.2214 (6.2214)   loss_c 3.4576	beta 0.750, 0.750, 0.500  loss_a 2.7748	mu 4.000000  loss_s -0.0111	loss_edge 0.0000	
Train: [3][0/12], lr: 0.03000	Time 6.286 (6.286)	Data 5.854 (5.854)	Prec@1 82.812 (82.812)	Prec@5 97.656 (97.656)	Loss 4.0385 (4.0385)   loss_c 1.2715	beta 0.750, 0.750, 0.500  loss_a 2.7797	mu 4.000000  loss_s -0.0128	loss_edge 0.0000	
Train: [4][0/12], lr: 0.03000	Time 5.971 (5.971)	Data 5.464 (5.464)	Prec@1 91.406 (91.406)	Prec@5 100.000 (100.000)	Loss 3.3338 (3.3338)   loss_c 0.5713	beta 0.750, 0.750, 0.500  loss_a 2.7726	mu 4.000000  loss_s -0.0101	loss_edge 0.0000	
Train: [5][0/12], lr: 0.03000	Time 6.109 (6.109)	Data 5.704 (5.704)	Prec@1 96.875 (96.875)	Prec@5 100.000 (100.000)	Loss 3.0013 (3.0013)   loss_c 0.2525	beta 0.750, 0.750, 0.500  loss_a 2.7632	mu 4.000000  loss_s -0.0144	loss_edge 0.0000	
Train: [6][0/12], lr: 0.03000	Time 5.684 (5.684)	Data 5.176 (5.176)	Prec@1 97.656 (97.656)	Prec@5 100.000 (100.000)	Loss 2.9480 (2.9480)   loss_c 0.2055	beta 0.750, 0.750, 0.500  loss_a 2.7573	mu 4.000000  loss_s -0.0149	loss_edge 0.0000	
Train: [7][0/12], lr: 0.03000	Time 5.896 (5.896)	Data 5.514 (5.514)	Prec@1 97.656 (97.656)	Prec@5 100.000 (100.000)	Loss 2.9275 (2.9275)   loss_c 0.1228	beta 0.750, 0.750, 0.500  loss_a 2.8151	mu 4.000000  loss_s -0.0105	loss_edge 0.0000	
Train: [8][0/12], lr: 0.03000	Time 6.204 (6.204)	Data 5.783 (5.783)	Prec@1 95.312 (95.312)	Prec@5 100.000 (100.000)	Loss 3.0678 (3.0678)   loss_c 0.2060	beta 0.750, 0.750, 0.500  loss_a 2.8742	mu 4.000000  loss_s -0.0124	loss_edge 0.0000	
Train: [9][0/12], lr: 0.03000	Time 6.244 (6.244)	Data 5.786 (5.786)	Prec@1 98.438 (98.438)	Prec@5 100.000 (100.000)	Loss 2.8671 (2.8671)   loss_c 0.1243	beta 0.750, 0.750, 0.500  loss_a 2.7532	mu 4.000000  loss_s -0.0104	loss_edge 0.0000	
Train: [10][0/12], lr: 0.00300	Time 6.269 (6.269)	Data 5.820 (5.820)	Prec@1 98.438 (98.438)	Prec@5 100.000 (100.000)	Loss 2.8691 (2.8691)   loss_c 0.1344	beta 0.750, 0.750, 0.500  loss_a 2.7460	mu 4.000000  loss_s -0.0114	loss_edge 0.0001	
Train: [11][0/12], lr: 0.00300	Time 6.034 (6.034)	Data 5.572 (5.572)	Prec@1 98.438 (98.438)	Prec@5 100.000 (100.000)	Loss 2.7590 (2.7590)   loss_c 0.1056	beta 0.750, 0.750, 0.500  loss_a 2.6622	mu 4.000000  loss_s -0.0089	loss_edge 0.0001	
Train: [12][0/12], lr: 0.00300	Time 5.986 (5.986)	Data 5.528 (5.528)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.6481 (2.6481)   loss_c 0.0485	beta 0.750, 0.750, 0.500  loss_a 2.6081	mu 4.000000  loss_s -0.0085	loss_edge 0.0001	
Train: [13][0/12], lr: 0.00300	Time 5.658 (5.658)	Data 5.196 (5.196)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.7445 (2.7445)   loss_c 0.0343	beta 0.750, 0.750, 0.500  loss_a 2.7190	mu 4.000000  loss_s -0.0088	loss_edge 0.0000	
Train: [14][0/12], lr: 0.00300	Time 5.889 (5.889)	Data 5.447 (5.447)	Prec@1 97.656 (97.656)	Prec@5 100.000 (100.000)	Loss 2.7806 (2.7806)   loss_c 0.1118	beta 0.750, 0.750, 0.500  loss_a 2.6768	mu 4.000000  loss_s -0.0080	loss_edge 0.0000	
Train: [15][0/12], lr: 0.00300	Time 5.672 (5.672)	Data 5.197 (5.197)	Prec@1 98.438 (98.438)	Prec@5 100.000 (100.000)	Loss 2.7117 (2.7117)   loss_c 0.0706	beta 0.750, 0.750, 0.500  loss_a 2.6493	mu 4.000000  loss_s -0.0083	loss_edge 0.0000	
Train: [16][0/12], lr: 0.00300	Time 6.024 (6.024)	Data 5.589 (5.589)	Prec@1 97.656 (97.656)	Prec@5 100.000 (100.000)	Loss 2.7785 (2.7785)   loss_c 0.0832	beta 0.750, 0.750, 0.500  loss_a 2.7024	mu 4.000000  loss_s -0.0072	loss_edge 0.0000	
Train: [17][0/12], lr: 0.00300	Time 6.259 (6.259)	Data 5.790 (5.790)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.7161 (2.7161)   loss_c 0.0278	beta 0.750, 0.750, 0.500  loss_a 2.6991	mu 4.000000  loss_s -0.0109	loss_edge 0.0000	
Train: [18][0/12], lr: 0.00300	Time 5.934 (5.934)	Data 5.451 (5.451)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 2.7747 (2.7747)   loss_c 0.1184	beta 0.750, 0.750, 0.500  loss_a 2.6662	mu 4.000000  loss_s -0.0100	loss_edge 0.0000	
Train: [19][0/12], lr: 0.00300	Time 5.896 (5.896)	Data 5.456 (5.456)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.6931 (2.6931)   loss_c 0.0190	beta 0.750, 0.750, 0.500  loss_a 2.6808	mu 4.000000  loss_s -0.0067	loss_edge 0.0000	
Train: [20][0/12], lr: 0.00030	Time 5.602 (5.602)	Data 5.159 (5.159)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.7232 (2.7232)   loss_c 0.0503	beta 0.750, 0.750, 0.500  loss_a 2.6820	mu 4.000000  loss_s -0.0091	loss_edge 0.0000	
Train: [21][0/12], lr: 0.00030	Time 6.435 (6.435)	Data 6.021 (6.021)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.6686 (2.6686)   loss_c 0.0267	beta 0.750, 0.750, 0.500  loss_a 2.6487	mu 4.000000  loss_s -0.0070	loss_edge 0.0000	
Train: [22][0/12], lr: 0.00030	Time 6.182 (6.182)	Data 5.732 (5.732)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 2.7430 (2.7430)   loss_c 0.0574	beta 0.750, 0.750, 0.500  loss_a 2.6931	mu 4.000000  loss_s -0.0076	loss_edge 0.0000	
Train: [23][0/12], lr: 0.00030	Time 5.672 (5.672)	Data 5.198 (5.198)	Prec@1 97.656 (97.656)	Prec@5 100.000 (100.000)	Loss 2.7622 (2.7622)   loss_c 0.0837	beta 0.750, 0.750, 0.500  loss_a 2.6852	mu 4.000000  loss_s -0.0068	loss_edge 0.0001	
Train: [24][0/12], lr: 0.00030	Time 6.162 (6.162)	Data 5.706 (5.706)	Prec@1 97.656 (97.656)	Prec@5 100.000 (100.000)	Loss 2.9073 (2.9073)   loss_c 0.1395	beta 0.750, 0.750, 0.500  loss_a 2.7767	mu 4.000000  loss_s -0.0089	loss_edge 0.0000	
Train: [25][0/12], lr: 0.00030	Time 6.351 (6.351)	Data 5.876 (5.876)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.7577 (2.7577)   loss_c 0.0462	beta 0.750, 0.750, 0.500  loss_a 2.7226	mu 4.000000  loss_s -0.0111	loss_edge 0.0000	
Train: [26][0/12], lr: 0.00030	Time 5.682 (5.682)	Data 5.209 (5.209)	Prec@1 98.438 (98.438)	Prec@5 100.000 (100.000)	Loss 2.7600 (2.7600)   loss_c 0.0353	beta 0.750, 0.750, 0.500  loss_a 2.7342	mu 4.000000  loss_s -0.0095	loss_edge 0.0001	
Train: [27][0/12], lr: 0.00030	Time 5.634 (5.634)	Data 5.205 (5.205)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 2.8225 (2.8225)   loss_c 0.0842	beta 0.750, 0.750, 0.500  loss_a 2.7463	mu 4.000000  loss_s -0.0081	loss_edge 0.0000	
Train: [28][0/12], lr: 0.00030	Time 5.724 (5.724)	Data 5.304 (5.304)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.6575 (2.6575)   loss_c 0.0210	beta 0.750, 0.750, 0.500  loss_a 2.6465	mu 4.000000  loss_s -0.0101	loss_edge 0.0001	
Train: [29][0/12], lr: 0.00030	Time 5.757 (5.757)	Data 5.282 (5.282)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.6973 (2.6973)   loss_c 0.0238	beta 0.750, 0.750, 0.500  loss_a 2.6814	mu 4.000000  loss_s -0.0079	loss_edge 0.0001	
Train: [30][0/12], lr: 0.00030	Time 5.617 (5.617)	Data 5.228 (5.228)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 2.7860 (2.7860)   loss_c 0.0609	beta 0.750, 0.750, 0.500  loss_a 2.7341	mu 4.000000  loss_s -0.0090	loss_edge 0.0001	
total time: 674.643 