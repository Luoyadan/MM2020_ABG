Train: [1][0/12], lr: 0.03000	Time 10.779 (10.779)	Data 3.939 (3.939)	Prec@1 15.625 (15.625)	Prec@5 42.969 (42.969)	Loss 3.8838 (3.8838)   loss_c 2.4848	beta 0.750, 0.750, 0.500  loss_a 1.3863	gamma 0.003000  loss_e 4.2073	
Train: [2][0/12], lr: 0.02071	Time 4.651 (4.651)	Data 4.564 (4.564)	Prec@1 17.969 (17.969)	Prec@5 65.625 (65.625)	Loss 3.6749 (3.6749)   loss_c 2.3485	beta 0.750, 0.750, 0.500  loss_a 1.3146	gamma 0.003000  loss_e 3.9109	
Train: [3][0/12], lr: 0.01803	Time 4.130 (4.130)	Data 4.018 (4.018)	Prec@1 39.844 (39.844)	Prec@5 75.781 (75.781)	Loss 3.2501 (3.2501)   loss_c 1.9245	beta 0.750, 0.750, 0.500  loss_a 1.3153	gamma 0.003000  loss_e 3.4183	
Train: [4][0/12], lr: 0.01603	Time 4.445 (4.445)	Data 4.327 (4.327)	Prec@1 57.812 (57.812)	Prec@5 94.531 (94.531)	Loss 2.5048 (2.5048)   loss_c 1.1838	beta 0.750, 0.750, 0.500  loss_a 1.3128	gamma 0.003000  loss_e 2.7441	
Train: [5][0/12], lr: 0.01449	Time 4.610 (4.610)	Data 4.523 (4.523)	Prec@1 87.500 (87.500)	Prec@5 98.438 (98.438)	Loss 1.8721 (1.8721)   loss_c 0.5568	beta 0.750, 0.750, 0.500  loss_a 1.3099	gamma 0.003000  loss_e 1.8218	
Train: [6][0/12], lr: 0.01325	Time 4.287 (4.287)	Data 4.201 (4.201)	Prec@1 89.062 (89.062)	Prec@5 99.219 (99.219)	Loss 1.7022 (1.7022)   loss_c 0.4034	beta 0.750, 0.750, 0.500  loss_a 1.2947	gamma 0.003000  loss_e 1.3723	
Train: [7][0/12], lr: 0.01224	Time 4.559 (4.559)	Data 4.480 (4.480)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)	Loss 1.5478 (1.5478)   loss_c 0.2777	beta 0.750, 0.750, 0.500  loss_a 1.2666	gamma 0.003000  loss_e 1.1335	
Train: [8][0/12], lr: 0.01139	Time 4.283 (4.283)	Data 4.218 (4.218)	Prec@1 93.750 (93.750)	Prec@5 99.219 (99.219)	Loss 1.5269 (1.5269)   loss_c 0.2623	beta 0.750, 0.750, 0.500  loss_a 1.2614	gamma 0.003000  loss_e 1.0760	
Train: [9][0/12], lr: 0.01066	Time 4.311 (4.311)	Data 4.259 (4.259)	Prec@1 93.750 (93.750)	Prec@5 99.219 (99.219)	Loss 1.4977 (1.4977)   loss_c 0.2435	beta 0.750, 0.750, 0.500  loss_a 1.2512	gamma 0.003000  loss_e 0.9866	
Train: [10][0/12], lr: 0.01004	Time 4.403 (4.403)	Data 4.309 (4.309)	Prec@1 95.312 (95.312)	Prec@5 100.000 (100.000)	Loss 1.4683 (1.4683)   loss_c 0.2043	beta 0.750, 0.750, 0.500  loss_a 1.2612	gamma 0.003000  loss_e 0.8955	
Train: [11][0/12], lr: 0.00949	Time 4.746 (4.746)	Data 4.662 (4.662)	Prec@1 96.875 (96.875)	Prec@5 100.000 (100.000)	Loss 1.3714 (1.3714)   loss_c 0.1516	beta 0.750, 0.750, 0.500  loss_a 1.2174	gamma 0.003000  loss_e 0.8009	
Train: [12][0/12], lr: 0.00901	Time 4.368 (4.368)	Data 4.309 (4.309)	Prec@1 98.438 (98.438)	Prec@5 100.000 (100.000)	Loss 1.3444 (1.3444)   loss_c 0.1057	beta 0.750, 0.750, 0.500  loss_a 1.2365	gamma 0.003000  loss_e 0.7426	
Train: [13][0/12], lr: 0.00858	Time 4.319 (4.319)	Data 4.253 (4.253)	Prec@1 98.438 (98.438)	Prec@5 100.000 (100.000)	Loss 1.3474 (1.3474)   loss_c 0.0769	beta 0.750, 0.750, 0.500  loss_a 1.2685	gamma 0.003000  loss_e 0.6487	
Train: [14][0/12], lr: 0.00820	Time 4.445 (4.445)	Data 4.365 (4.365)	Prec@1 98.438 (98.438)	Prec@5 100.000 (100.000)	Loss 1.2969 (1.2969)   loss_c 0.0664	beta 0.750, 0.750, 0.500  loss_a 1.2288	gamma 0.003000  loss_e 0.5830	
Train: [15][0/12], lr: 0.00785	Time 4.350 (4.350)	Data 4.298 (4.298)	Prec@1 98.438 (98.438)	Prec@5 100.000 (100.000)	Loss 1.3229 (1.3229)   loss_c 0.0722	beta 0.750, 0.750, 0.500  loss_a 1.2491	gamma 0.003000  loss_e 0.5481	
Train: [16][0/12], lr: 0.00754	Time 4.288 (4.288)	Data 4.223 (4.223)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 1.3195 (1.3195)   loss_c 0.0798	beta 0.750, 0.750, 0.500  loss_a 1.2380	gamma 0.003000  loss_e 0.5606	
Train: [17][0/12], lr: 0.00725	Time 4.193 (4.193)	Data 4.115 (4.115)	Prec@1 98.438 (98.438)	Prec@5 100.000 (100.000)	Loss 1.2917 (1.2917)   loss_c 0.0695	beta 0.750, 0.750, 0.500  loss_a 1.2204	gamma 0.003000  loss_e 0.5882	
Train: [18][0/12], lr: 0.00699	Time 4.446 (4.446)	Data 4.382 (4.382)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 1.3502 (1.3502)   loss_c 0.0734	beta 0.750, 0.750, 0.500  loss_a 1.2750	gamma 0.003000  loss_e 0.5774	
Train: [19][0/12], lr: 0.00675	Time 4.246 (4.246)	Data 4.195 (4.195)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 1.2858 (1.2858)   loss_c 0.0670	beta 0.750, 0.750, 0.500  loss_a 1.2170	gamma 0.003000  loss_e 0.6089	
Train: [20][0/12], lr: 0.00653	Time 4.396 (4.396)	Data 4.321 (4.321)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 1.3283 (1.3283)   loss_c 0.0517	beta 0.750, 0.750, 0.500  loss_a 1.2751	gamma 0.003000  loss_e 0.4675	
Train: [21][0/12], lr: 0.00632	Time 4.431 (4.431)	Data 4.361 (4.361)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 1.3347 (1.3347)   loss_c 0.0500	beta 0.750, 0.750, 0.500  loss_a 1.2832	gamma 0.003000  loss_e 0.4891	
Train: [22][0/12], lr: 0.00613	Time 4.469 (4.469)	Data 4.387 (4.387)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 1.3013 (1.3013)   loss_c 0.0629	beta 0.750, 0.750, 0.500  loss_a 1.2366	gamma 0.003000  loss_e 0.5853	
Train: [23][0/12], lr: 0.00595	Time 4.562 (4.562)	Data 4.499 (4.499)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 1.2616 (1.2616)   loss_c 0.0382	beta 0.750, 0.750, 0.500  loss_a 1.2218	gamma 0.003000  loss_e 0.5332	
Train: [24][0/12], lr: 0.00579	Time 4.536 (4.536)	Data 4.471 (4.471)	Prec@1 98.438 (98.438)	Prec@5 100.000 (100.000)	Loss 1.2593 (1.2593)   loss_c 0.0568	beta 0.750, 0.750, 0.500  loss_a 1.2013	gamma 0.003000  loss_e 0.4122	
Train: [25][0/12], lr: 0.00563	Time 4.494 (4.494)	Data 4.444 (4.444)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 1.2567 (1.2567)   loss_c 0.0329	beta 0.750, 0.750, 0.500  loss_a 1.2226	gamma 0.003000  loss_e 0.4010	
Train: [26][0/12], lr: 0.00548	Time 4.202 (4.202)	Data 4.113 (4.113)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 1.3078 (1.3078)   loss_c 0.0671	beta 0.750, 0.750, 0.500  loss_a 1.2392	gamma 0.003000  loss_e 0.4863	
Train: [27][0/12], lr: 0.00535	Time 4.513 (4.513)	Data 4.439 (4.439)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 1.2846 (1.2846)   loss_c 0.0627	beta 0.750, 0.750, 0.500  loss_a 1.2206	gamma 0.003000  loss_e 0.4512	
Train: [28][0/12], lr: 0.00522	Time 4.358 (4.358)	Data 4.307 (4.307)	Prec@1 98.438 (98.438)	Prec@5 100.000 (100.000)	Loss 1.2659 (1.2659)   loss_c 0.0609	beta 0.750, 0.750, 0.500  loss_a 1.2037	gamma 0.003000  loss_e 0.4207	
Train: [29][0/12], lr: 0.00509	Time 4.356 (4.356)	Data 4.285 (4.285)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 1.2893 (1.2893)   loss_c 0.0343	beta 0.750, 0.750, 0.500  loss_a 1.2537	gamma 0.003000  loss_e 0.4547	
Train: [30][0/12], lr: 0.00498	Time 4.348 (4.348)	Data 4.289 (4.289)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 1.3127 (1.3127)   loss_c 0.0518	beta 0.750, 0.750, 0.500  loss_a 1.2598	gamma 0.003000  loss_e 0.3961	
total time: 471.499 