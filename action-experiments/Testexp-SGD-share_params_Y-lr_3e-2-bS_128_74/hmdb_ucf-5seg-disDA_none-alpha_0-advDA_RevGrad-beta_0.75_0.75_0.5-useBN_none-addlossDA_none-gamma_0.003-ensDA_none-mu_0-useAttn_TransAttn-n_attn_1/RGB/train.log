Train: [1][0/12], lr: 0.03000	Time 8.700 (8.700)	Data 5.088 (5.088)	Prec@1 6.250 (6.250)	Prec@5 33.594 (33.594)	Loss 5.2576 (5.2576)   loss_c 2.4850	beta 0.750, 0.750, 0.500  loss_a 0.6931	
Train: [2][0/12], lr: 0.03000	Time 5.253 (5.253)	Data 5.204 (5.204)	Prec@1 23.438 (23.438)	Prec@5 57.812 (57.812)	Loss 4.9815 (4.9815)   loss_c 2.3425	beta 0.750, 0.750, 0.500  loss_a 0.6598	
Train: [3][0/12], lr: 0.03000	Time 4.982 (4.982)	Data 4.940 (4.940)	Prec@1 39.062 (39.062)	Prec@5 81.250 (81.250)	Loss 4.4262 (4.4262)   loss_c 1.7931	beta 0.750, 0.750, 0.500  loss_a 0.6583	
Train: [4][0/12], lr: 0.03000	Time 5.146 (5.146)	Data 5.096 (5.096)	Prec@1 54.688 (54.688)	Prec@5 90.625 (90.625)	Loss 4.0894 (4.0894)   loss_c 1.4615	beta 0.750, 0.750, 0.500  loss_a 0.6570	
Train: [5][0/12], lr: 0.03000	Time 5.281 (5.281)	Data 5.232 (5.232)	Prec@1 57.031 (57.031)	Prec@5 97.656 (97.656)	Loss 3.7935 (3.7935)   loss_c 1.1759	beta 0.750, 0.750, 0.500  loss_a 0.6544	
Train: [6][0/12], lr: 0.03000	Time 5.684 (5.684)	Data 5.637 (5.637)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)	Loss 2.9933 (2.9933)   loss_c 0.3762	beta 0.750, 0.750, 0.500  loss_a 0.6543	
Train: [7][0/12], lr: 0.03000	Time 4.903 (4.903)	Data 4.857 (4.857)	Prec@1 95.312 (95.312)	Prec@5 100.000 (100.000)	Loss 2.7043 (2.7043)   loss_c 0.2568	beta 0.750, 0.750, 0.500  loss_a 0.6119	
Train: [8][0/12], lr: 0.03000	Time 5.130 (5.130)	Data 5.069 (5.069)	Prec@1 96.094 (96.094)	Prec@5 100.000 (100.000)	Loss 2.7644 (2.7644)   loss_c 0.1745	beta 0.750, 0.750, 0.500  loss_a 0.6475	
Train: [9][0/12], lr: 0.03000	Time 5.108 (5.108)	Data 5.068 (5.068)	Prec@1 95.312 (95.312)	Prec@5 100.000 (100.000)	Loss 2.6914 (2.6914)   loss_c 0.1548	beta 0.750, 0.750, 0.500  loss_a 0.6342	
Train: [10][0/12], lr: 0.00300	Time 5.157 (5.157)	Data 5.111 (5.111)	Prec@1 96.875 (96.875)	Prec@5 100.000 (100.000)	Loss 2.7648 (2.7648)   loss_c 0.1002	beta 0.750, 0.750, 0.500  loss_a 0.6662	
Train: [11][0/12], lr: 0.00300	Time 4.945 (4.945)	Data 4.900 (4.900)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.5383 (2.5383)   loss_c 0.0599	beta 0.750, 0.750, 0.500  loss_a 0.6196	
Train: [12][0/12], lr: 0.00300	Time 5.616 (5.616)	Data 5.576 (5.576)	Prec@1 98.438 (98.438)	Prec@5 100.000 (100.000)	Loss 2.5233 (2.5233)   loss_c 0.0621	beta 0.750, 0.750, 0.500  loss_a 0.6153	
Train: [13][0/12], lr: 0.00300	Time 5.329 (5.329)	Data 5.277 (5.277)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.5209 (2.5209)   loss_c 0.0541	beta 0.750, 0.750, 0.500  loss_a 0.6167	
Train: [14][0/12], lr: 0.00300	Time 5.037 (5.037)	Data 4.990 (4.990)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 2.5295 (2.5295)   loss_c 0.0530	beta 0.750, 0.750, 0.500  loss_a 0.6191	
Train: [15][0/12], lr: 0.00300	Time 4.947 (4.947)	Data 4.906 (4.906)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 2.5985 (2.5985)   loss_c 0.0515	beta 0.750, 0.750, 0.500  loss_a 0.6367	
Train: [16][0/12], lr: 0.00300	Time 5.171 (5.171)	Data 5.127 (5.127)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 2.6608 (2.6608)   loss_c 0.0704	beta 0.750, 0.750, 0.500  loss_a 0.6476	
Train: [17][0/12], lr: 0.00300	Time 4.972 (4.972)	Data 4.924 (4.924)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.6192 (2.6192)   loss_c 0.0410	beta 0.750, 0.750, 0.500  loss_a 0.6446	
Train: [18][0/12], lr: 0.00300	Time 5.395 (5.395)	Data 5.342 (5.342)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.5977 (2.5977)   loss_c 0.0450	beta 0.750, 0.750, 0.500  loss_a 0.6382	
Train: [19][0/12], lr: 0.00300	Time 5.211 (5.211)	Data 5.159 (5.159)	Prec@1 98.438 (98.438)	Prec@5 100.000 (100.000)	Loss 2.6046 (2.6046)   loss_c 0.0525	beta 0.750, 0.750, 0.500  loss_a 0.6380	
Train: [20][0/12], lr: 0.00030	Time 5.130 (5.130)	Data 5.056 (5.056)	Prec@1 98.438 (98.438)	Prec@5 100.000 (100.000)	Loss 2.6209 (2.6209)   loss_c 0.0513	beta 0.750, 0.750, 0.500  loss_a 0.6424	
Train: [21][0/12], lr: 0.00030	Time 4.954 (4.954)	Data 4.899 (4.899)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 2.5856 (2.5856)   loss_c 0.0412	beta 0.750, 0.750, 0.500  loss_a 0.6361	
Train: [22][0/12], lr: 0.00030	Time 5.762 (5.762)	Data 5.703 (5.703)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.5503 (2.5503)   loss_c 0.0307	beta 0.750, 0.750, 0.500  loss_a 0.6299	
Train: [23][0/12], lr: 0.00030	Time 5.482 (5.482)	Data 5.447 (5.447)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.6001 (2.6001)   loss_c 0.0463	beta 0.750, 0.750, 0.500  loss_a 0.6384	
Train: [24][0/12], lr: 0.00030	Time 5.180 (5.180)	Data 5.129 (5.129)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.6541 (2.6541)   loss_c 0.0261	beta 0.750, 0.750, 0.500  loss_a 0.6570	
Train: [25][0/12], lr: 0.00030	Time 5.039 (5.039)	Data 4.993 (4.993)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 2.6048 (2.6048)   loss_c 0.0452	beta 0.750, 0.750, 0.500  loss_a 0.6399	
Train: [26][0/12], lr: 0.00030	Time 5.058 (5.058)	Data 5.010 (5.010)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 2.5729 (2.5729)   loss_c 0.0454	beta 0.750, 0.750, 0.500  loss_a 0.6319	
Train: [27][0/12], lr: 0.00030	Time 5.105 (5.105)	Data 5.064 (5.064)	Prec@1 97.656 (97.656)	Prec@5 100.000 (100.000)	Loss 2.5752 (2.5752)   loss_c 0.0583	beta 0.750, 0.750, 0.500  loss_a 0.6292	
Train: [28][0/12], lr: 0.00030	Time 4.890 (4.890)	Data 4.839 (4.839)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)	Loss 2.5826 (2.5826)   loss_c 0.0328	beta 0.750, 0.750, 0.500  loss_a 0.6374	
Train: [29][0/12], lr: 0.00030	Time 5.507 (5.507)	Data 5.457 (5.457)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)	Loss 2.5694 (2.5694)   loss_c 0.0363	beta 0.750, 0.750, 0.500  loss_a 0.6333	
Train: [30][0/12], lr: 0.00030	Time 5.621 (5.621)	Data 5.576 (5.576)	Prec@1 98.438 (98.438)	Prec@5 100.000 (100.000)	Loss 2.6389 (2.6389)   loss_c 0.0524	beta 0.750, 0.750, 0.500  loss_a 0.6466	
total time: 667.807 