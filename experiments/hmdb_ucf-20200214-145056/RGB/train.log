Train: [1][0/45], lr: 0.03000	Time 6.232 (6.232)	Data 2.344 (2.344)	Prec@1 15.625 (15.625)	Prec@5 65.625 (65.625)	Loss 4.5748 (4.5748)   loss_c 2.4826	beta 0.750, 0.750, 0.500  loss_a 2.0797	gamma 0.003000  loss_e 4.2073	
Train: [1][10/45], lr: 0.02331	Time 0.044 (0.611)	Data 0.000 (0.213)	Prec@1 9.375 (18.466)	Prec@5 46.875 (51.420)	Loss 4.7061 (4.6704)   loss_c 2.5811	beta 0.750, 0.750, 0.500  loss_a 2.0782	gamma 0.003000  loss_e 3.6645	
Train: [1][20/45], lr: 0.02242	Time 0.079 (0.405)	Data 0.000 (0.162)	Prec@1 28.125 (19.345)	Prec@5 59.375 (51.042)	Loss 4.2855 (4.7651)   loss_c 2.6782	beta 0.750, 0.750, 0.500  loss_a 2.0760	gamma 0.003000  loss_e 3.6301	
Train: [1][30/45], lr: 0.02162	Time 0.082 (0.344)	Data 0.000 (0.157)	Prec@1 3.125 (20.464)	Prec@5 43.750 (52.823)	Loss 4.7338 (4.9126)   loss_c 2.8267	beta 0.750, 0.750, 0.500  loss_a 2.0750	gamma 0.003000  loss_e 3.6472	
Train: [1][40/45], lr: 0.02087	Time 0.086 (0.316)	Data 0.000 (0.152)	Prec@1 12.500 (19.665)	Prec@5 43.750 (51.601)	Loss 4.6187 (4.8244)   loss_c 2.7386	beta 0.750, 0.750, 0.500  loss_a 2.0746	gamma 0.003000  loss_e 3.7414	
Train: [2][0/45], lr: 0.02052	Time 2.770 (2.770)	Data 2.669 (2.669)	Prec@1 21.875 (21.875)	Prec@5 59.375 (59.375)	Loss 4.4465 (4.4465)   loss_c 2.3615	beta 0.750, 0.750, 0.500  loss_a 2.0730	gamma 0.003000  loss_e 3.9806	
Train: [2][10/45], lr: 0.01986	Time 0.063 (0.417)	Data 0.000 (0.355)	Prec@1 15.625 (25.284)	Prec@5 40.625 (59.943)	Loss 4.6433 (4.4791)   loss_c 2.3944	beta 0.750, 0.750, 0.500  loss_a 2.0729	gamma 0.003000  loss_e 3.9434	
Train: [2][20/45], lr: 0.01925	Time 0.049 (0.308)	Data 0.000 (0.247)	Prec@1 25.000 (21.577)	Prec@5 50.000 (59.226)	Loss 4.4587 (4.4903)   loss_c 2.4054	beta 0.750, 0.750, 0.500  loss_a 2.0729	gamma 0.003000  loss_e 4.0006	
Train: [2][30/45], lr: 0.01867	Time 0.046 (0.276)	Data 0.000 (0.215)	Prec@1 31.250 (21.169)	Prec@5 46.875 (57.359)	Loss 4.4413 (4.5130)   loss_c 2.4282	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9754	
Train: [2][40/45], lr: 0.01814	Time 0.896 (0.277)	Data 0.814 (0.216)	Prec@1 12.500 (22.332)	Prec@5 46.875 (57.774)	Loss 4.6517 (4.4938)   loss_c 2.4091	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9643	
Train: [3][0/45], lr: 0.01789	Time 2.762 (2.762)	Data 2.670 (2.670)	Prec@1 18.750 (18.750)	Prec@5 53.125 (53.125)	Loss 4.5297 (4.5297)   loss_c 2.4447	beta 0.750, 0.750, 0.500  loss_a 2.0729	gamma 0.003000  loss_e 4.0090	
Train: [3][10/45], lr: 0.01740	Time 0.084 (0.419)	Data 0.000 (0.351)	Prec@1 21.875 (19.602)	Prec@5 59.375 (55.966)	Loss 4.4715 (4.5053)   loss_c 2.4203	beta 0.750, 0.750, 0.500  loss_a 2.0729	gamma 0.003000  loss_e 4.0469	
Train: [3][20/45], lr: 0.01695	Time 0.051 (0.308)	Data 0.000 (0.242)	Prec@1 40.625 (20.982)	Prec@5 78.125 (57.143)	Loss 4.2157 (4.4861)   loss_c 2.4010	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0647	
Train: [3][30/45], lr: 0.01652	Time 0.040 (0.273)	Data 0.000 (0.207)	Prec@1 37.500 (22.581)	Prec@5 62.500 (58.065)	Loss 4.3229 (4.4630)   loss_c 2.3780	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0459	
Train: [3][40/45], lr: 0.01612	Time 1.057 (0.278)	Data 0.980 (0.215)	Prec@1 18.750 (22.485)	Prec@5 65.625 (57.165)	Loss 4.5127 (4.4618)   loss_c 2.3770	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0086	
Train: [4][0/45], lr: 0.01593	Time 2.798 (2.798)	Data 2.689 (2.689)	Prec@1 9.375 (9.375)	Prec@5 34.375 (34.375)	Loss 4.6755 (4.6755)   loss_c 2.5909	beta 0.750, 0.750, 0.500  loss_a 2.0727	gamma 0.003000  loss_e 3.9724	
Train: [4][10/45], lr: 0.01556	Time 0.065 (0.424)	Data 0.000 (0.357)	Prec@1 28.125 (21.023)	Prec@5 65.625 (52.557)	Loss 4.3572 (4.4837)   loss_c 2.3989	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0288	
Train: [4][20/45], lr: 0.01521	Time 0.054 (0.318)	Data 0.000 (0.255)	Prec@1 31.250 (23.810)	Prec@5 56.250 (55.952)	Loss 4.3830 (4.4561)   loss_c 2.3712	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0348	
Train: [4][30/45], lr: 0.01487	Time 0.039 (0.280)	Data 0.000 (0.221)	Prec@1 25.000 (23.387)	Prec@5 65.625 (56.956)	Loss 4.3876 (4.4576)   loss_c 2.3728	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0185	
Train: [4][40/45], lr: 0.01456	Time 0.826 (0.278)	Data 0.748 (0.219)	Prec@1 21.875 (23.171)	Prec@5 71.875 (57.470)	Loss 4.4728 (4.4567)   loss_c 2.3719	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0153	
Train: [5][0/45], lr: 0.01441	Time 2.782 (2.782)	Data 2.671 (2.671)	Prec@1 12.500 (12.500)	Prec@5 62.500 (62.500)	Loss 4.5264 (4.5264)   loss_c 2.4415	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0321	
Train: [5][10/45], lr: 0.01411	Time 0.048 (0.428)	Data 0.000 (0.354)	Prec@1 31.250 (23.011)	Prec@5 71.875 (60.511)	Loss 4.3021 (4.4261)   loss_c 2.3412	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0353	
Train: [5][20/45], lr: 0.01383	Time 0.044 (0.308)	Data 0.000 (0.241)	Prec@1 25.000 (22.768)	Prec@5 68.750 (59.524)	Loss 4.3540 (4.4341)   loss_c 2.3492	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0224	
Train: [5][30/45], lr: 0.01356	Time 0.043 (0.273)	Data 0.000 (0.211)	Prec@1 21.875 (23.992)	Prec@5 62.500 (58.468)	Loss 4.4272 (4.4311)   loss_c 2.3462	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0128	
Train: [5][40/45], lr: 0.01331	Time 0.810 (0.275)	Data 0.732 (0.214)	Prec@1 18.750 (23.171)	Prec@5 37.500 (57.774)	Loss 4.5607 (4.4418)   loss_c 2.3570	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0084	
Train: [6][0/45], lr: 0.01319	Time 2.742 (2.742)	Data 2.652 (2.652)	Prec@1 6.250 (6.250)	Prec@5 43.750 (43.750)	Loss 4.5982 (4.5982)   loss_c 2.5133	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0266	
Train: [6][10/45], lr: 0.01295	Time 0.087 (0.421)	Data 0.000 (0.349)	Prec@1 25.000 (22.443)	Prec@5 56.250 (60.511)	Loss 4.4334 (4.4262)   loss_c 2.3414	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0056	
Train: [6][20/45], lr: 0.01272	Time 0.041 (0.308)	Data 0.000 (0.244)	Prec@1 28.125 (23.214)	Prec@5 46.875 (57.589)	Loss 4.4555 (4.4442)   loss_c 2.3594	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0048	
Train: [6][30/45], lr: 0.01250	Time 0.042 (0.275)	Data 0.000 (0.213)	Prec@1 18.750 (22.077)	Prec@5 50.000 (55.847)	Loss 4.5217 (4.4578)   loss_c 2.3730	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0107	
Train: [6][40/45], lr: 0.01228	Time 1.018 (0.280)	Data 0.936 (0.218)	Prec@1 25.000 (23.171)	Prec@5 56.250 (56.555)	Loss 4.4183 (4.4446)   loss_c 2.3598	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0120	
Train: [7][0/45], lr: 0.01218	Time 2.708 (2.708)	Data 2.603 (2.603)	Prec@1 18.750 (18.750)	Prec@5 50.000 (50.000)	Loss 4.4592 (4.4592)   loss_c 2.3743	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0144	
Train: [7][10/45], lr: 0.01198	Time 0.077 (0.414)	Data 0.000 (0.342)	Prec@1 28.125 (20.739)	Prec@5 75.000 (59.091)	Loss 4.3561 (4.4588)   loss_c 2.3740	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0052	
Train: [7][20/45], lr: 0.01179	Time 0.041 (0.309)	Data 0.000 (0.244)	Prec@1 18.750 (23.363)	Prec@5 50.000 (58.185)	Loss 4.5025 (4.4364)   loss_c 2.3515	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0095	
Train: [7][30/45], lr: 0.01160	Time 0.044 (0.276)	Data 0.000 (0.212)	Prec@1 18.750 (22.480)	Prec@5 46.875 (57.964)	Loss 4.5009 (4.4429)   loss_c 2.3581	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0043	
Train: [7][40/45], lr: 0.01143	Time 1.093 (0.281)	Data 1.017 (0.218)	Prec@1 21.875 (23.018)	Prec@5 53.125 (57.317)	Loss 4.5175 (4.4442)   loss_c 2.3594	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0023	
Train: [8][0/45], lr: 0.01134	Time 2.809 (2.809)	Data 2.682 (2.682)	Prec@1 21.875 (21.875)	Prec@5 53.125 (53.125)	Loss 4.5376 (4.5376)   loss_c 2.4527	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9989	
Train: [8][10/45], lr: 0.01117	Time 0.089 (0.421)	Data 0.000 (0.351)	Prec@1 25.000 (23.580)	Prec@5 62.500 (59.943)	Loss 4.3711 (4.4268)   loss_c 2.3420	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9966	
Train: [8][20/45], lr: 0.01101	Time 0.041 (0.307)	Data 0.000 (0.242)	Prec@1 21.875 (22.619)	Prec@5 53.125 (57.887)	Loss 4.4509 (4.4441)   loss_c 2.3593	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9885	
Train: [8][30/45], lr: 0.01085	Time 0.045 (0.273)	Data 0.000 (0.209)	Prec@1 34.375 (23.790)	Prec@5 43.750 (56.754)	Loss 4.3618 (4.4366)   loss_c 2.3518	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9929	
Train: [8][40/45], lr: 0.01070	Time 1.089 (0.279)	Data 1.023 (0.216)	Prec@1 21.875 (23.628)	Prec@5 62.500 (57.851)	Loss 4.4200 (4.4329)   loss_c 2.3482	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9931	
Train: [9][0/45], lr: 0.01062	Time 2.768 (2.768)	Data 2.645 (2.645)	Prec@1 31.250 (31.250)	Prec@5 59.375 (59.375)	Loss 4.3423 (4.3423)   loss_c 2.2576	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9915	
Train: [9][10/45], lr: 0.01048	Time 0.082 (0.417)	Data 0.000 (0.348)	Prec@1 34.375 (22.159)	Prec@5 84.375 (58.523)	Loss 4.2613 (4.4538)   loss_c 2.3691	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9948	
Train: [9][20/45], lr: 0.01034	Time 0.067 (0.309)	Data 0.000 (0.243)	Prec@1 25.000 (23.214)	Prec@5 59.375 (57.738)	Loss 4.4331 (4.4444)   loss_c 2.3596	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9957	
Train: [9][30/45], lr: 0.01020	Time 0.045 (0.275)	Data 0.000 (0.210)	Prec@1 21.875 (22.782)	Prec@5 56.250 (58.065)	Loss 4.4724 (4.4490)   loss_c 2.3643	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9940	
Train: [9][40/45], lr: 0.01007	Time 0.727 (0.273)	Data 0.638 (0.207)	Prec@1 18.750 (23.018)	Prec@5 62.500 (58.079)	Loss 4.4341 (4.4454)   loss_c 2.3606	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9947	
Train: [10][0/45], lr: 0.01000	Time 2.766 (2.766)	Data 2.666 (2.666)	Prec@1 25.000 (25.000)	Prec@5 65.625 (65.625)	Loss 4.3610 (4.3610)   loss_c 2.2763	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9885	
Train: [10][10/45], lr: 0.00987	Time 0.061 (0.425)	Data 0.000 (0.362)	Prec@1 25.000 (21.307)	Prec@5 62.500 (56.534)	Loss 4.4526 (4.4696)   loss_c 2.3849	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9939	
Train: [10][20/45], lr: 0.00975	Time 0.046 (0.307)	Data 0.000 (0.250)	Prec@1 31.250 (24.107)	Prec@5 65.625 (58.929)	Loss 4.3525 (4.4343)   loss_c 2.3495	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9946	
Train: [10][30/45], lr: 0.00963	Time 0.044 (0.272)	Data 0.000 (0.216)	Prec@1 12.500 (23.992)	Prec@5 43.750 (57.964)	Loss 4.6014 (4.4378)   loss_c 2.3530	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9897	
Train: [10][40/45], lr: 0.00952	Time 1.026 (0.279)	Data 0.948 (0.222)	Prec@1 15.625 (23.247)	Prec@5 43.750 (57.774)	Loss 4.5666 (4.4413)   loss_c 2.3565	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9859	
Train: [11][0/45], lr: 0.00946	Time 2.782 (2.782)	Data 2.681 (2.681)	Prec@1 37.500 (37.500)	Prec@5 62.500 (62.500)	Loss 4.2669 (4.2669)   loss_c 2.1822	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9828	
Train: [11][10/45], lr: 0.00935	Time 0.089 (0.423)	Data 0.000 (0.354)	Prec@1 28.125 (25.852)	Prec@5 65.625 (61.932)	Loss 4.3677 (4.4056)   loss_c 2.3209	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9727	
Train: [11][20/45], lr: 0.00924	Time 0.043 (0.319)	Data 0.000 (0.257)	Prec@1 12.500 (24.256)	Prec@5 40.625 (57.143)	Loss 4.5599 (4.4264)   loss_c 2.3417	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9704	
Train: [11][30/45], lr: 0.00913	Time 0.055 (0.284)	Data 0.000 (0.224)	Prec@1 18.750 (23.589)	Prec@5 59.375 (58.468)	Loss 4.4854 (4.4320)   loss_c 2.3473	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9717	
Train: [11][40/45], lr: 0.00903	Time 1.067 (0.288)	Data 0.987 (0.228)	Prec@1 12.500 (22.485)	Prec@5 59.375 (57.851)	Loss 4.5515 (4.4445)   loss_c 2.3598	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9731	
Train: [12][0/45], lr: 0.00898	Time 2.794 (2.794)	Data 2.695 (2.695)	Prec@1 18.750 (18.750)	Prec@5 50.000 (50.000)	Loss 4.4871 (4.4871)   loss_c 2.4023	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9999	
Train: [12][10/45], lr: 0.00888	Time 0.079 (0.425)	Data 0.000 (0.363)	Prec@1 21.875 (23.011)	Prec@5 53.125 (56.250)	Loss 4.4159 (4.4388)   loss_c 2.3541	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9866	
Train: [12][20/45], lr: 0.00879	Time 0.047 (0.310)	Data 0.000 (0.250)	Prec@1 15.625 (21.726)	Prec@5 62.500 (56.399)	Loss 4.5038 (4.4596)   loss_c 2.3749	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9854	
Train: [12][30/45], lr: 0.00869	Time 0.043 (0.276)	Data 0.000 (0.215)	Prec@1 21.875 (22.480)	Prec@5 59.375 (58.165)	Loss 4.5109 (4.4450)   loss_c 2.3603	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9865	
Train: [12][40/45], lr: 0.00860	Time 1.236 (0.286)	Data 1.158 (0.223)	Prec@1 34.375 (22.866)	Prec@5 59.375 (58.537)	Loss 4.3629 (4.4401)   loss_c 2.3553	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9858	
Train: [13][0/45], lr: 0.00856	Time 2.753 (2.753)	Data 2.634 (2.634)	Prec@1 43.750 (43.750)	Prec@5 65.625 (65.625)	Loss 4.2711 (4.2711)   loss_c 2.1864	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9766	
Train: [13][10/45], lr: 0.00847	Time 0.086 (0.419)	Data 0.000 (0.349)	Prec@1 25.000 (23.864)	Prec@5 65.625 (58.239)	Loss 4.3356 (4.4305)   loss_c 2.3458	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9764	
Train: [13][20/45], lr: 0.00838	Time 0.044 (0.308)	Data 0.000 (0.243)	Prec@1 18.750 (22.619)	Prec@5 59.375 (57.440)	Loss 4.4942 (4.4470)   loss_c 2.3623	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9767	
Train: [13][30/45], lr: 0.00830	Time 0.043 (0.275)	Data 0.000 (0.213)	Prec@1 21.875 (22.379)	Prec@5 43.750 (56.956)	Loss 4.4977 (4.4477)   loss_c 2.3630	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9782	
Train: [13][40/45], lr: 0.00822	Time 1.013 (0.281)	Data 0.940 (0.219)	Prec@1 28.125 (23.018)	Prec@5 53.125 (57.241)	Loss 4.4801 (4.4412)   loss_c 2.3565	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9788	
Train: [14][0/45], lr: 0.00818	Time 2.728 (2.728)	Data 2.622 (2.622)	Prec@1 25.000 (25.000)	Prec@5 75.000 (75.000)	Loss 4.3526 (4.3526)   loss_c 2.2680	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9668	
Train: [14][10/45], lr: 0.00810	Time 0.087 (0.418)	Data 0.000 (0.343)	Prec@1 25.000 (24.716)	Prec@5 59.375 (61.932)	Loss 4.4385 (4.4223)   loss_c 2.3376	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9729	
Train: [14][20/45], lr: 0.00802	Time 0.042 (0.310)	Data 0.000 (0.245)	Prec@1 15.625 (22.619)	Prec@5 62.500 (59.821)	Loss 4.5547 (4.4468)   loss_c 2.3621	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9737	
Train: [14][30/45], lr: 0.00794	Time 0.051 (0.277)	Data 0.000 (0.215)	Prec@1 25.000 (22.581)	Prec@5 62.500 (58.367)	Loss 4.4202 (4.4491)   loss_c 2.3644	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9756	
Train: [14][40/45], lr: 0.00787	Time 0.829 (0.276)	Data 0.753 (0.214)	Prec@1 18.750 (22.942)	Prec@5 43.750 (57.927)	Loss 4.5410 (4.4427)   loss_c 2.3580	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9776	
Train: [15][0/45], lr: 0.00783	Time 2.796 (2.796)	Data 2.686 (2.686)	Prec@1 25.000 (25.000)	Prec@5 59.375 (59.375)	Loss 4.4375 (4.4375)   loss_c 2.3528	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9838	
Train: [15][10/45], lr: 0.00776	Time 0.095 (0.421)	Data 0.000 (0.354)	Prec@1 25.000 (25.568)	Prec@5 62.500 (58.239)	Loss 4.3275 (4.4160)   loss_c 2.3313	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9796	
Train: [15][20/45], lr: 0.00769	Time 0.047 (0.307)	Data 0.000 (0.244)	Prec@1 21.875 (24.702)	Prec@5 62.500 (58.333)	Loss 4.3893 (4.4169)   loss_c 2.3322	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9782	
Train: [15][30/45], lr: 0.00762	Time 0.052 (0.273)	Data 0.000 (0.211)	Prec@1 15.625 (22.681)	Prec@5 46.875 (56.653)	Loss 4.5577 (4.4496)   loss_c 2.3649	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9772	
Train: [15][40/45], lr: 0.00755	Time 0.963 (0.276)	Data 0.882 (0.214)	Prec@1 12.500 (22.485)	Prec@5 56.250 (57.622)	Loss 4.5664 (4.4442)   loss_c 2.3595	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9795	
Train: [16][0/45], lr: 0.00752	Time 2.797 (2.797)	Data 2.695 (2.695)	Prec@1 15.625 (15.625)	Prec@5 65.625 (65.625)	Loss 4.4684 (4.4684)   loss_c 2.3836	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9888	
Train: [16][10/45], lr: 0.00746	Time 0.080 (0.416)	Data 0.000 (0.354)	Prec@1 18.750 (21.591)	Prec@5 40.625 (59.091)	Loss 4.5690 (4.4503)   loss_c 2.3656	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9866	
Train: [16][20/45], lr: 0.00739	Time 0.046 (0.308)	Data 0.000 (0.246)	Prec@1 21.875 (23.810)	Prec@5 65.625 (58.631)	Loss 4.4670 (4.4290)   loss_c 2.3443	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9860	
Train: [16][30/45], lr: 0.00733	Time 0.049 (0.275)	Data 0.000 (0.215)	Prec@1 21.875 (23.891)	Prec@5 56.250 (58.770)	Loss 4.4620 (4.4309)   loss_c 2.3462	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9832	
Train: [16][40/45], lr: 0.00727	Time 1.148 (0.283)	Data 1.072 (0.224)	Prec@1 9.375 (22.713)	Prec@5 53.125 (57.927)	Loss 4.6193 (4.4437)   loss_c 2.3590	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9813	
Train: [17][0/45], lr: 0.00724	Time 2.816 (2.816)	Data 2.723 (2.723)	Prec@1 25.000 (25.000)	Prec@5 50.000 (50.000)	Loss 4.4467 (4.4467)   loss_c 2.3620	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9746	
Train: [17][10/45], lr: 0.00718	Time 0.058 (0.429)	Data 0.000 (0.359)	Prec@1 28.125 (25.284)	Prec@5 65.625 (63.068)	Loss 4.3830 (4.4042)   loss_c 2.3195	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9780	
Train: [17][20/45], lr: 0.00712	Time 0.046 (0.309)	Data 0.000 (0.241)	Prec@1 18.750 (23.363)	Prec@5 34.375 (59.375)	Loss 4.5835 (4.4450)   loss_c 2.3603	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9779	
Train: [17][30/45], lr: 0.00706	Time 0.040 (0.274)	Data 0.000 (0.210)	Prec@1 21.875 (22.984)	Prec@5 59.375 (59.375)	Loss 4.4233 (4.4441)   loss_c 2.3594	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9790	
Train: [17][40/45], lr: 0.00700	Time 0.815 (0.275)	Data 0.730 (0.212)	Prec@1 12.500 (23.095)	Prec@5 31.250 (58.918)	Loss 4.6194 (4.4414)   loss_c 2.3566	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9800	
Train: [18][0/45], lr: 0.00698	Time 2.724 (2.724)	Data 2.648 (2.648)	Prec@1 21.875 (21.875)	Prec@5 65.625 (65.625)	Loss 4.3915 (4.3915)   loss_c 2.3068	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9840	
Train: [18][10/45], lr: 0.00692	Time 0.090 (0.415)	Data 0.000 (0.356)	Prec@1 25.000 (25.284)	Prec@5 59.375 (56.818)	Loss 4.4601 (4.4270)   loss_c 2.3423	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9814	
Train: [18][20/45], lr: 0.00687	Time 0.049 (0.306)	Data 0.000 (0.246)	Prec@1 21.875 (22.917)	Prec@5 53.125 (56.548)	Loss 4.5164 (4.4520)   loss_c 2.3673	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9806	
Train: [18][30/45], lr: 0.00681	Time 0.048 (0.269)	Data 0.000 (0.210)	Prec@1 25.000 (22.681)	Prec@5 46.875 (57.157)	Loss 4.4583 (4.4495)   loss_c 2.3648	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9820	
Train: [18][40/45], lr: 0.00676	Time 1.395 (0.286)	Data 1.320 (0.225)	Prec@1 21.875 (22.713)	Prec@5 46.875 (57.927)	Loss 4.4640 (4.4459)   loss_c 2.3612	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9831	
Train: [19][0/45], lr: 0.00674	Time 2.754 (2.754)	Data 2.643 (2.643)	Prec@1 21.875 (21.875)	Prec@5 62.500 (62.500)	Loss 4.4668 (4.4668)   loss_c 2.3821	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9813	
Train: [19][10/45], lr: 0.00669	Time 0.095 (0.423)	Data 0.000 (0.349)	Prec@1 25.000 (25.000)	Prec@5 59.375 (59.943)	Loss 4.4143 (4.4192)   loss_c 2.3345	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9819	
Train: [19][20/45], lr: 0.00664	Time 0.048 (0.310)	Data 0.000 (0.244)	Prec@1 34.375 (24.107)	Prec@5 56.250 (57.292)	Loss 4.3118 (4.4261)   loss_c 2.3414	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9807	
Train: [19][30/45], lr: 0.00659	Time 0.045 (0.278)	Data 0.000 (0.216)	Prec@1 12.500 (23.185)	Prec@5 43.750 (56.653)	Loss 4.5632 (4.4410)   loss_c 2.3563	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9810	
Train: [19][40/45], lr: 0.00654	Time 1.259 (0.288)	Data 1.184 (0.228)	Prec@1 18.750 (23.399)	Prec@5 50.000 (57.393)	Loss 4.4897 (4.4351)   loss_c 2.3504	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9802	
Train: [20][0/45], lr: 0.00652	Time 2.802 (2.802)	Data 2.675 (2.675)	Prec@1 18.750 (18.750)	Prec@5 65.625 (65.625)	Loss 4.5012 (4.5012)   loss_c 2.4165	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9792	
Train: [20][10/45], lr: 0.00647	Time 0.072 (0.415)	Data 0.000 (0.345)	Prec@1 31.250 (23.864)	Prec@5 53.125 (59.659)	Loss 4.3466 (4.4213)   loss_c 2.3366	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9788	
Train: [20][20/45], lr: 0.00642	Time 0.043 (0.309)	Data 0.000 (0.243)	Prec@1 34.375 (23.512)	Prec@5 56.250 (58.780)	Loss 4.3356 (4.4275)   loss_c 2.3428	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9799	
Train: [20][30/45], lr: 0.00638	Time 0.041 (0.274)	Data 0.000 (0.210)	Prec@1 21.875 (22.984)	Prec@5 53.125 (57.762)	Loss 4.4777 (4.4446)   loss_c 2.3599	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9807	
Train: [20][40/45], lr: 0.00633	Time 1.100 (0.280)	Data 1.022 (0.217)	Prec@1 43.750 (23.247)	Prec@5 81.250 (58.232)	Loss 4.1305 (4.4391)   loss_c 2.3544	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9812	
Train: [21][0/45], lr: 0.00631	Time 2.761 (2.761)	Data 2.658 (2.658)	Prec@1 18.750 (18.750)	Prec@5 50.000 (50.000)	Loss 4.4655 (4.4655)   loss_c 2.3807	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9792	
Train: [21][10/45], lr: 0.00627	Time 0.083 (0.419)	Data 0.000 (0.352)	Prec@1 15.625 (21.591)	Prec@5 59.375 (59.091)	Loss 4.4913 (4.4560)   loss_c 2.3712	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9807	
Train: [21][20/45], lr: 0.00622	Time 0.053 (0.308)	Data 0.000 (0.244)	Prec@1 15.625 (21.726)	Prec@5 53.125 (57.440)	Loss 4.5272 (4.4558)   loss_c 2.3711	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9814	
Train: [21][30/45], lr: 0.00618	Time 0.039 (0.274)	Data 0.000 (0.210)	Prec@1 6.250 (22.782)	Prec@5 34.375 (58.266)	Loss 4.7314 (4.4440)   loss_c 2.3593	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9820	
Train: [21][40/45], lr: 0.00614	Time 0.811 (0.275)	Data 0.732 (0.211)	Prec@1 37.500 (23.247)	Prec@5 59.375 (58.079)	Loss 4.3207 (4.4403)   loss_c 2.3556	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9817	
Train: [22][0/45], lr: 0.00612	Time 2.755 (2.755)	Data 2.663 (2.663)	Prec@1 31.250 (31.250)	Prec@5 62.500 (62.500)	Loss 4.3499 (4.3499)   loss_c 2.2652	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9796	
Train: [22][10/45], lr: 0.00608	Time 0.078 (0.418)	Data 0.000 (0.339)	Prec@1 12.500 (19.886)	Prec@5 50.000 (53.977)	Loss 4.5590 (4.4903)   loss_c 2.4056	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9806	
Train: [22][20/45], lr: 0.00604	Time 0.045 (0.308)	Data 0.000 (0.238)	Prec@1 40.625 (21.429)	Prec@5 71.875 (58.185)	Loss 4.2120 (4.4528)   loss_c 2.3680	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9833	
Train: [22][30/45], lr: 0.00600	Time 0.046 (0.272)	Data 0.000 (0.207)	Prec@1 15.625 (22.077)	Prec@5 56.250 (57.359)	Loss 4.4643 (4.4491)   loss_c 2.3644	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9844	
Train: [22][40/45], lr: 0.00596	Time 0.851 (0.273)	Data 0.776 (0.211)	Prec@1 28.125 (22.713)	Prec@5 62.500 (58.003)	Loss 4.3875 (4.4462)   loss_c 2.3614	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9843	
Train: [23][0/45], lr: 0.00594	Time 2.863 (2.863)	Data 2.762 (2.762)	Prec@1 18.750 (18.750)	Prec@5 59.375 (59.375)	Loss 4.4973 (4.4973)   loss_c 2.4126	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9839	
Train: [23][10/45], lr: 0.00591	Time 0.060 (0.426)	Data 0.000 (0.361)	Prec@1 31.250 (24.432)	Prec@5 43.750 (55.966)	Loss 4.4217 (4.4262)   loss_c 2.3415	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9790	
Train: [23][20/45], lr: 0.00587	Time 0.046 (0.309)	Data 0.000 (0.249)	Prec@1 21.875 (24.702)	Prec@5 53.125 (56.548)	Loss 4.5222 (4.4247)   loss_c 2.3400	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9772	
Train: [23][30/45], lr: 0.00583	Time 0.043 (0.273)	Data 0.000 (0.213)	Prec@1 12.500 (23.589)	Prec@5 50.000 (56.956)	Loss 4.5837 (4.4363)   loss_c 2.3516	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9752	
Train: [23][40/45], lr: 0.00579	Time 0.916 (0.275)	Data 0.832 (0.214)	Prec@1 28.125 (23.018)	Prec@5 71.875 (58.079)	Loss 4.2750 (4.4385)   loss_c 2.3538	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9744	
Train: [24][0/45], lr: 0.00578	Time 2.730 (2.730)	Data 2.628 (2.628)	Prec@1 15.625 (15.625)	Prec@5 53.125 (53.125)	Loss 4.5369 (4.5369)   loss_c 2.4522	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9733	
Train: [24][10/45], lr: 0.00574	Time 0.061 (0.415)	Data 0.000 (0.348)	Prec@1 37.500 (25.000)	Prec@5 78.125 (59.943)	Loss 4.2355 (4.4055)   loss_c 2.3208	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9770	
Train: [24][20/45], lr: 0.00571	Time 0.039 (0.306)	Data 0.000 (0.241)	Prec@1 21.875 (25.446)	Prec@5 68.750 (59.375)	Loss 4.4532 (4.4155)   loss_c 2.3308	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9755	
Train: [24][30/45], lr: 0.00567	Time 0.053 (0.274)	Data 0.000 (0.210)	Prec@1 28.125 (23.790)	Prec@5 46.875 (58.468)	Loss 4.4081 (4.4323)   loss_c 2.3476	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9744	
Train: [24][40/45], lr: 0.00564	Time 1.116 (0.280)	Data 1.034 (0.217)	Prec@1 25.000 (23.171)	Prec@5 53.125 (58.003)	Loss 4.4310 (4.4388)   loss_c 2.3541	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9748	
Train: [25][0/45], lr: 0.00562	Time 2.787 (2.787)	Data 2.685 (2.685)	Prec@1 28.125 (28.125)	Prec@5 62.500 (62.500)	Loss 4.3730 (4.3730)   loss_c 2.2883	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9773	
Train: [25][10/45], lr: 0.00559	Time 0.080 (0.416)	Data 0.000 (0.350)	Prec@1 31.250 (24.148)	Prec@5 56.250 (55.966)	Loss 4.4015 (4.4280)   loss_c 2.3433	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9807	
Train: [25][20/45], lr: 0.00556	Time 0.040 (0.309)	Data 0.000 (0.245)	Prec@1 18.750 (23.214)	Prec@5 43.750 (57.738)	Loss 4.5596 (4.4394)   loss_c 2.3546	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9792	
Train: [25][30/45], lr: 0.00552	Time 0.046 (0.271)	Data 0.000 (0.210)	Prec@1 15.625 (23.790)	Prec@5 59.375 (58.065)	Loss 4.5056 (4.4323)   loss_c 2.3476	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9777	
Train: [25][40/45], lr: 0.00549	Time 1.068 (0.280)	Data 0.993 (0.219)	Prec@1 28.125 (22.866)	Prec@5 50.000 (58.308)	Loss 4.4463 (4.4387)   loss_c 2.3540	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9777	
Train: [26][0/45], lr: 0.00548	Time 2.718 (2.718)	Data 2.616 (2.616)	Prec@1 18.750 (18.750)	Prec@5 62.500 (62.500)	Loss 4.4537 (4.4537)   loss_c 2.3690	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9790	
Train: [26][10/45], lr: 0.00544	Time 0.070 (0.414)	Data 0.000 (0.346)	Prec@1 15.625 (22.159)	Prec@5 53.125 (56.534)	Loss 4.5167 (4.4531)   loss_c 2.3684	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9804	
Train: [26][20/45], lr: 0.00541	Time 0.048 (0.309)	Data 0.000 (0.247)	Prec@1 21.875 (22.768)	Prec@5 62.500 (58.185)	Loss 4.4541 (4.4410)   loss_c 2.3563	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9804	
Train: [26][30/45], lr: 0.00538	Time 0.039 (0.278)	Data 0.000 (0.217)	Prec@1 28.125 (24.093)	Prec@5 71.875 (58.569)	Loss 4.3485 (4.4316)   loss_c 2.3469	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9802	
Train: [26][40/45], lr: 0.00535	Time 1.147 (0.284)	Data 1.071 (0.223)	Prec@1 12.500 (23.476)	Prec@5 46.875 (58.460)	Loss 4.6236 (4.4364)   loss_c 2.3517	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9796	
Train: [27][0/45], lr: 0.00534	Time 2.835 (2.835)	Data 2.719 (2.719)	Prec@1 18.750 (18.750)	Prec@5 53.125 (53.125)	Loss 4.4837 (4.4837)   loss_c 2.3990	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9816	
Train: [27][10/45], lr: 0.00531	Time 0.071 (0.471)	Data 0.000 (0.395)	Prec@1 18.750 (22.159)	Prec@5 53.125 (60.227)	Loss 4.4659 (4.4442)   loss_c 2.3595	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9798	
Train: [27][20/45], lr: 0.00528	Time 0.048 (0.345)	Data 0.000 (0.278)	Prec@1 37.500 (23.065)	Prec@5 68.750 (60.119)	Loss 4.2665 (4.4282)   loss_c 2.3435	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9807	
Train: [27][30/45], lr: 0.00525	Time 0.045 (0.313)	Data 0.000 (0.249)	Prec@1 21.875 (23.488)	Prec@5 56.250 (57.762)	Loss 4.4505 (4.4379)   loss_c 2.3532	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9807	
Train: [27][40/45], lr: 0.00522	Time 0.947 (0.308)	Data 0.862 (0.243)	Prec@1 12.500 (22.713)	Prec@5 37.500 (57.088)	Loss 4.6005 (4.4459)   loss_c 2.3612	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9806	
Train: [28][0/45], lr: 0.00521	Time 3.112 (3.112)	Data 3.005 (3.005)	Prec@1 25.000 (25.000)	Prec@5 59.375 (59.375)	Loss 4.4390 (4.4390)   loss_c 2.3543	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9820	
Train: [28][10/45], lr: 0.00518	Time 0.063 (0.446)	Data 0.000 (0.382)	Prec@1 12.500 (21.307)	Prec@5 50.000 (59.375)	Loss 4.5577 (4.4389)   loss_c 2.3542	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9818	
Train: [28][20/45], lr: 0.00515	Time 0.042 (0.335)	Data 0.000 (0.276)	Prec@1 18.750 (22.470)	Prec@5 59.375 (59.077)	Loss 4.5009 (4.4397)   loss_c 2.3550	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9806	
Train: [28][30/45], lr: 0.00513	Time 0.050 (0.293)	Data 0.000 (0.234)	Prec@1 18.750 (22.681)	Prec@5 56.250 (58.065)	Loss 4.4650 (4.4457)   loss_c 2.3610	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9807	
Train: [28][40/45], lr: 0.00510	Time 2.708 (0.343)	Data 2.629 (0.284)	Prec@1 21.875 (22.713)	Prec@5 62.500 (57.774)	Loss 4.4165 (4.4474)   loss_c 2.3627	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9810	
Train: [29][0/45], lr: 0.00509	Time 2.796 (2.796)	Data 2.703 (2.703)	Prec@1 18.750 (18.750)	Prec@5 53.125 (53.125)	Loss 4.4933 (4.4933)   loss_c 2.4086	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9803	
Train: [29][10/45], lr: 0.00506	Time 0.066 (0.447)	Data 0.000 (0.375)	Prec@1 37.500 (25.568)	Prec@5 62.500 (59.659)	Loss 4.3009 (4.4196)   loss_c 2.3349	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9802	
Train: [29][20/45], lr: 0.00503	Time 0.041 (0.328)	Data 0.000 (0.264)	Prec@1 15.625 (22.917)	Prec@5 43.750 (58.780)	Loss 4.5527 (4.4441)   loss_c 2.3594	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9795	
Train: [29][30/45], lr: 0.00501	Time 0.048 (0.289)	Data 0.000 (0.227)	Prec@1 15.625 (22.379)	Prec@5 56.250 (58.165)	Loss 4.4885 (4.4495)   loss_c 2.3648	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9802	
Train: [29][40/45], lr: 0.00498	Time 1.046 (0.291)	Data 0.962 (0.230)	Prec@1 18.750 (22.637)	Prec@5 56.250 (57.698)	Loss 4.4210 (4.4437)   loss_c 2.3589	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9808	
Train: [30][0/45], lr: 0.00497	Time 2.775 (2.775)	Data 2.665 (2.665)	Prec@1 34.375 (34.375)	Prec@5 71.875 (71.875)	Loss 4.2650 (4.2650)   loss_c 2.1803	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9863	
Train: [30][10/45], lr: 0.00494	Time 0.084 (0.449)	Data 0.000 (0.380)	Prec@1 28.125 (23.580)	Prec@5 71.875 (60.511)	Loss 4.3922 (4.4271)   loss_c 2.3424	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9812	
Train: [30][20/45], lr: 0.00492	Time 0.043 (0.325)	Data 0.000 (0.260)	Prec@1 34.375 (23.214)	Prec@5 50.000 (58.929)	Loss 4.3781 (4.4351)   loss_c 2.3504	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9804	
Train: [30][30/45], lr: 0.00490	Time 0.042 (0.287)	Data 0.000 (0.222)	Prec@1 31.250 (23.286)	Prec@5 68.750 (58.871)	Loss 4.3017 (4.4314)   loss_c 2.3467	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9795	
Train: [30][40/45], lr: 0.00487	Time 1.858 (0.363)	Data 1.781 (0.299)	Prec@1 9.375 (22.866)	Prec@5 62.500 (58.003)	Loss 4.4836 (4.4405)   loss_c 2.3558	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9793	
total time: 472.187 