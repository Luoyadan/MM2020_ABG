Train: [1][40/45], lr: 0.02087	Time 0.086 (0.316)	Data 0.000 (0.152)	Prec@1 12.500 (19.665)	Prec@5 43.750 (51.601)	Loss 4.6187 (4.8244)   loss_c 2.7386	beta 0.750, 0.750, 0.500  loss_a 2.0746	gamma 0.003000  loss_e 3.7414	
Train: [2][40/45], lr: 0.01814	Time 0.896 (0.277)	Data 0.814 (0.216)	Prec@1 12.500 (22.332)	Prec@5 46.875 (57.774)	Loss 4.6517 (4.4938)   loss_c 2.4091	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9643	
Train: [3][40/45], lr: 0.01612	Time 1.057 (0.278)	Data 0.980 (0.215)	Prec@1 18.750 (22.485)	Prec@5 65.625 (57.165)	Loss 4.5127 (4.4618)   loss_c 2.3770	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0086	
Train: [4][40/45], lr: 0.01456	Time 0.826 (0.278)	Data 0.748 (0.219)	Prec@1 21.875 (23.171)	Prec@5 71.875 (57.470)	Loss 4.4728 (4.4567)   loss_c 2.3719	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0153	
Train: [5][40/45], lr: 0.01331	Time 0.810 (0.275)	Data 0.732 (0.214)	Prec@1 18.750 (23.171)	Prec@5 37.500 (57.774)	Loss 4.5607 (4.4418)   loss_c 2.3570	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0084	
Train: [6][40/45], lr: 0.01228	Time 1.018 (0.280)	Data 0.936 (0.218)	Prec@1 25.000 (23.171)	Prec@5 56.250 (56.555)	Loss 4.4183 (4.4446)   loss_c 2.3598	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0120	
Train: [7][40/45], lr: 0.01143	Time 1.093 (0.281)	Data 1.017 (0.218)	Prec@1 21.875 (23.018)	Prec@5 53.125 (57.317)	Loss 4.5175 (4.4442)   loss_c 2.3594	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0023	
Train: [8][40/45], lr: 0.01070	Time 1.089 (0.279)	Data 1.023 (0.216)	Prec@1 21.875 (23.628)	Prec@5 62.500 (57.851)	Loss 4.4200 (4.4329)   loss_c 2.3482	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9931	
Train: [9][40/45], lr: 0.01007	Time 0.727 (0.273)	Data 0.638 (0.207)	Prec@1 18.750 (23.018)	Prec@5 62.500 (58.079)	Loss 4.4341 (4.4454)   loss_c 2.3606	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9947	
Train: [10][40/45], lr: 0.00952	Time 1.026 (0.279)	Data 0.948 (0.222)	Prec@1 15.625 (23.247)	Prec@5 43.750 (57.774)	Loss 4.5666 (4.4413)   loss_c 2.3565	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9859	
Train: [11][40/45], lr: 0.00903	Time 1.067 (0.288)	Data 0.987 (0.228)	Prec@1 12.500 (22.485)	Prec@5 59.375 (57.851)	Loss 4.5515 (4.4445)   loss_c 2.3598	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9731	
Train: [12][40/45], lr: 0.00860	Time 1.236 (0.286)	Data 1.158 (0.223)	Prec@1 34.375 (22.866)	Prec@5 59.375 (58.537)	Loss 4.3629 (4.4401)   loss_c 2.3553	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9858	
Train: [13][40/45], lr: 0.00822	Time 1.013 (0.281)	Data 0.940 (0.219)	Prec@1 28.125 (23.018)	Prec@5 53.125 (57.241)	Loss 4.4801 (4.4412)   loss_c 2.3565	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9788	
Train: [14][40/45], lr: 0.00787	Time 0.829 (0.276)	Data 0.753 (0.214)	Prec@1 18.750 (22.942)	Prec@5 43.750 (57.927)	Loss 4.5410 (4.4427)   loss_c 2.3580	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9776	
Train: [15][40/45], lr: 0.00755	Time 0.963 (0.276)	Data 0.882 (0.214)	Prec@1 12.500 (22.485)	Prec@5 56.250 (57.622)	Loss 4.5664 (4.4442)   loss_c 2.3595	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9795	
Train: [16][40/45], lr: 0.00727	Time 1.148 (0.283)	Data 1.072 (0.224)	Prec@1 9.375 (22.713)	Prec@5 53.125 (57.927)	Loss 4.6193 (4.4437)   loss_c 2.3590	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9813	
Train: [17][40/45], lr: 0.00700	Time 0.815 (0.275)	Data 0.730 (0.212)	Prec@1 12.500 (23.095)	Prec@5 31.250 (58.918)	Loss 4.6194 (4.4414)   loss_c 2.3566	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9800	
Train: [18][40/45], lr: 0.00676	Time 1.395 (0.286)	Data 1.320 (0.225)	Prec@1 21.875 (22.713)	Prec@5 46.875 (57.927)	Loss 4.4640 (4.4459)   loss_c 2.3612	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9831	
Train: [19][40/45], lr: 0.00654	Time 1.259 (0.288)	Data 1.184 (0.228)	Prec@1 18.750 (23.399)	Prec@5 50.000 (57.393)	Loss 4.4897 (4.4351)   loss_c 2.3504	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9802	
Train: [20][40/45], lr: 0.00633	Time 1.100 (0.280)	Data 1.022 (0.217)	Prec@1 43.750 (23.247)	Prec@5 81.250 (58.232)	Loss 4.1305 (4.4391)   loss_c 2.3544	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9812	
Train: [21][40/45], lr: 0.00614	Time 0.811 (0.275)	Data 0.732 (0.211)	Prec@1 37.500 (23.247)	Prec@5 59.375 (58.079)	Loss 4.3207 (4.4403)   loss_c 2.3556	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9817	
Train: [22][40/45], lr: 0.00596	Time 0.851 (0.273)	Data 0.776 (0.211)	Prec@1 28.125 (22.713)	Prec@5 62.500 (58.003)	Loss 4.3875 (4.4462)   loss_c 2.3614	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9843	
Train: [23][40/45], lr: 0.00579	Time 0.916 (0.275)	Data 0.832 (0.214)	Prec@1 28.125 (23.018)	Prec@5 71.875 (58.079)	Loss 4.2750 (4.4385)   loss_c 2.3538	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9744	
Train: [24][40/45], lr: 0.00564	Time 1.116 (0.280)	Data 1.034 (0.217)	Prec@1 25.000 (23.171)	Prec@5 53.125 (58.003)	Loss 4.4310 (4.4388)   loss_c 2.3541	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9748	
Train: [25][40/45], lr: 0.00549	Time 1.068 (0.280)	Data 0.993 (0.219)	Prec@1 28.125 (22.866)	Prec@5 50.000 (58.308)	Loss 4.4463 (4.4387)   loss_c 2.3540	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9777	
Train: [26][40/45], lr: 0.00535	Time 1.147 (0.284)	Data 1.071 (0.223)	Prec@1 12.500 (23.476)	Prec@5 46.875 (58.460)	Loss 4.6236 (4.4364)   loss_c 2.3517	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9796	
Train: [27][40/45], lr: 0.00522	Time 0.947 (0.308)	Data 0.862 (0.243)	Prec@1 12.500 (22.713)	Prec@5 37.500 (57.088)	Loss 4.6005 (4.4459)   loss_c 2.3612	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9806	
Train: [28][40/45], lr: 0.00510	Time 2.708 (0.343)	Data 2.629 (0.284)	Prec@1 21.875 (22.713)	Prec@5 62.500 (57.774)	Loss 4.4165 (4.4474)   loss_c 2.3627	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9810	
Train: [29][40/45], lr: 0.00498	Time 1.046 (0.291)	Data 0.962 (0.230)	Prec@1 18.750 (22.637)	Prec@5 56.250 (57.698)	Loss 4.4210 (4.4437)   loss_c 2.3589	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9808	
Train: [30][40/45], lr: 0.00487	Time 1.858 (0.363)	Data 1.781 (0.299)	Prec@1 9.375 (22.866)	Prec@5 62.500 (58.003)	Loss 4.4836 (4.4405)   loss_c 2.3558	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9793	
total time: 472.187 