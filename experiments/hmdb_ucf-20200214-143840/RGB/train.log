Train: [1][0/45], lr: 0.03000	Time 6.796 (6.796)	Data 2.479 (2.479)	Prec@1 15.625 (15.625)	Prec@5 65.625 (65.625)	Loss 4.5748 (4.5748)   loss_c 2.4826	beta 0.750, 0.750, 0.500  loss_a 2.0797	gamma 0.003000  loss_e 4.2073	
Train: [1][10/45], lr: 0.02331	Time 0.057 (0.689)	Data 0.000 (0.226)	Prec@1 9.375 (18.466)	Prec@5 46.875 (51.420)	Loss 4.7061 (4.6704)   loss_c 2.5811	beta 0.750, 0.750, 0.500  loss_a 2.0782	gamma 0.003000  loss_e 3.6645	
Train: [1][20/45], lr: 0.02242	Time 0.194 (0.440)	Data 0.000 (0.152)	Prec@1 28.125 (19.345)	Prec@5 59.375 (51.042)	Loss 4.2855 (4.7651)   loss_c 2.6782	beta 0.750, 0.750, 0.500  loss_a 2.0760	gamma 0.003000  loss_e 3.6301	
Train: [1][30/45], lr: 0.02162	Time 0.136 (0.383)	Data 0.000 (0.147)	Prec@1 3.125 (20.464)	Prec@5 43.750 (52.823)	Loss 4.7338 (4.9126)   loss_c 2.8267	beta 0.750, 0.750, 0.500  loss_a 2.0750	gamma 0.003000  loss_e 3.6472	
Train: [1][40/45], lr: 0.02087	Time 0.077 (0.340)	Data 0.000 (0.132)	Prec@1 12.500 (19.665)	Prec@5 43.750 (51.601)	Loss 4.6187 (4.8244)   loss_c 2.7386	beta 0.750, 0.750, 0.500  loss_a 2.0746	gamma 0.003000  loss_e 3.7414	
Train: [2][0/45], lr: 0.02052	Time 3.087 (3.087)	Data 2.908 (2.908)	Prec@1 21.875 (21.875)	Prec@5 59.375 (59.375)	Loss 4.4465 (4.4465)   loss_c 2.3615	beta 0.750, 0.750, 0.500  loss_a 2.0730	gamma 0.003000  loss_e 3.9806	
Train: [2][10/45], lr: 0.01986	Time 0.070 (0.459)	Data 0.002 (0.356)	Prec@1 15.625 (25.284)	Prec@5 40.625 (59.943)	Loss 4.6433 (4.4791)   loss_c 2.3944	beta 0.750, 0.750, 0.500  loss_a 2.0729	gamma 0.003000  loss_e 3.9434	
Train: [2][20/45], lr: 0.01925	Time 0.075 (0.327)	Data 0.000 (0.237)	Prec@1 25.000 (21.577)	Prec@5 50.000 (59.226)	Loss 4.4587 (4.4903)   loss_c 2.4054	beta 0.750, 0.750, 0.500  loss_a 2.0729	gamma 0.003000  loss_e 4.0006	
Train: [2][30/45], lr: 0.01867	Time 0.069 (0.291)	Data 0.000 (0.203)	Prec@1 31.250 (21.169)	Prec@5 46.875 (57.359)	Loss 4.4413 (4.5130)   loss_c 2.4282	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9754	
Train: [2][40/45], lr: 0.01814	Time 1.560 (0.307)	Data 1.462 (0.220)	Prec@1 12.500 (22.332)	Prec@5 46.875 (57.774)	Loss 4.6517 (4.4938)   loss_c 2.4091	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9643	
Train: [3][0/45], lr: 0.01789	Time 3.310 (3.310)	Data 3.020 (3.020)	Prec@1 18.750 (18.750)	Prec@5 53.125 (53.125)	Loss 4.5297 (4.5297)   loss_c 2.4447	beta 0.750, 0.750, 0.500  loss_a 2.0729	gamma 0.003000  loss_e 4.0090	
Train: [3][10/45], lr: 0.01740	Time 0.120 (0.464)	Data 0.001 (0.357)	Prec@1 21.875 (19.602)	Prec@5 59.375 (55.966)	Loss 4.4715 (4.5053)   loss_c 2.4203	beta 0.750, 0.750, 0.500  loss_a 2.0729	gamma 0.003000  loss_e 4.0469	
Train: [3][20/45], lr: 0.01695	Time 0.079 (0.331)	Data 0.000 (0.236)	Prec@1 40.625 (20.982)	Prec@5 78.125 (57.143)	Loss 4.2157 (4.4861)   loss_c 2.4010	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0647	
Train: [3][30/45], lr: 0.01652	Time 0.071 (0.288)	Data 0.000 (0.197)	Prec@1 37.500 (22.581)	Prec@5 62.500 (58.065)	Loss 4.3229 (4.4630)   loss_c 2.3780	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0459	
Train: [3][40/45], lr: 0.01612	Time 0.905 (0.286)	Data 0.792 (0.197)	Prec@1 18.750 (22.485)	Prec@5 65.625 (57.165)	Loss 4.5127 (4.4618)   loss_c 2.3770	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0086	
Train: [4][0/45], lr: 0.01593	Time 3.140 (3.140)	Data 2.953 (2.953)	Prec@1 9.375 (9.375)	Prec@5 34.375 (34.375)	Loss 4.6755 (4.6755)   loss_c 2.5909	beta 0.750, 0.750, 0.500  loss_a 2.0727	gamma 0.003000  loss_e 3.9724	
Train: [4][10/45], lr: 0.01556	Time 0.091 (0.456)	Data 0.000 (0.361)	Prec@1 28.125 (21.023)	Prec@5 65.625 (52.557)	Loss 4.3572 (4.4837)   loss_c 2.3989	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0288	
Train: [4][20/45], lr: 0.01521	Time 0.067 (0.329)	Data 0.000 (0.240)	Prec@1 31.250 (23.810)	Prec@5 56.250 (55.952)	Loss 4.3830 (4.4561)   loss_c 2.3712	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0348	
Train: [4][30/45], lr: 0.01487	Time 0.070 (0.283)	Data 0.000 (0.197)	Prec@1 25.000 (23.387)	Prec@5 65.625 (56.956)	Loss 4.3876 (4.4576)   loss_c 2.3728	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0185	
Train: [4][40/45], lr: 0.01456	Time 0.985 (0.287)	Data 0.878 (0.201)	Prec@1 21.875 (23.171)	Prec@5 71.875 (57.470)	Loss 4.4728 (4.4567)   loss_c 2.3719	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0153	
Train: [5][0/45], lr: 0.01441	Time 3.089 (3.089)	Data 2.928 (2.928)	Prec@1 12.500 (12.500)	Prec@5 62.500 (62.500)	Loss 4.5264 (4.5264)   loss_c 2.4415	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0321	
Train: [5][10/45], lr: 0.01411	Time 0.083 (0.456)	Data 0.001 (0.359)	Prec@1 31.250 (23.011)	Prec@5 71.875 (60.511)	Loss 4.3021 (4.4261)   loss_c 2.3412	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0353	
Train: [5][20/45], lr: 0.01383	Time 0.064 (0.327)	Data 0.000 (0.236)	Prec@1 25.000 (22.768)	Prec@5 68.750 (59.524)	Loss 4.3540 (4.4341)   loss_c 2.3492	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0224	
Train: [5][30/45], lr: 0.01356	Time 0.069 (0.283)	Data 0.000 (0.194)	Prec@1 21.875 (23.992)	Prec@5 62.500 (58.468)	Loss 4.4272 (4.4311)   loss_c 2.3462	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0128	
Train: [5][40/45], lr: 0.01331	Time 1.234 (0.292)	Data 1.129 (0.203)	Prec@1 18.750 (23.171)	Prec@5 37.500 (57.774)	Loss 4.5607 (4.4418)   loss_c 2.3570	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0084	
Train: [6][0/45], lr: 0.01319	Time 3.112 (3.112)	Data 2.956 (2.956)	Prec@1 6.250 (6.250)	Prec@5 43.750 (43.750)	Loss 4.5982 (4.5982)   loss_c 2.5133	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0266	
Train: [6][10/45], lr: 0.01295	Time 0.096 (0.455)	Data 0.001 (0.359)	Prec@1 25.000 (22.443)	Prec@5 56.250 (60.511)	Loss 4.4334 (4.4262)   loss_c 2.3414	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0056	
Train: [6][20/45], lr: 0.01272	Time 0.069 (0.330)	Data 0.001 (0.238)	Prec@1 28.125 (23.214)	Prec@5 46.875 (57.589)	Loss 4.4555 (4.4442)   loss_c 2.3594	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0048	
Train: [6][30/45], lr: 0.01250	Time 0.069 (0.289)	Data 0.000 (0.197)	Prec@1 18.750 (22.077)	Prec@5 50.000 (55.847)	Loss 4.5217 (4.4578)   loss_c 2.3730	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0107	
Train: [6][40/45], lr: 0.01228	Time 1.152 (0.294)	Data 1.061 (0.205)	Prec@1 25.000 (23.171)	Prec@5 56.250 (56.555)	Loss 4.4183 (4.4446)   loss_c 2.3598	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0120	
Train: [7][0/45], lr: 0.01218	Time 2.982 (2.982)	Data 2.847 (2.847)	Prec@1 18.750 (18.750)	Prec@5 50.000 (50.000)	Loss 4.4592 (4.4592)   loss_c 2.3743	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0144	
Train: [7][10/45], lr: 0.01198	Time 0.119 (0.443)	Data 0.000 (0.345)	Prec@1 28.125 (20.739)	Prec@5 75.000 (59.091)	Loss 4.3561 (4.4588)   loss_c 2.3740	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0052	
Train: [7][20/45], lr: 0.01179	Time 0.074 (0.327)	Data 0.000 (0.236)	Prec@1 18.750 (23.363)	Prec@5 50.000 (58.185)	Loss 4.5025 (4.4364)   loss_c 2.3515	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0095	
Train: [7][30/45], lr: 0.01160	Time 0.078 (0.292)	Data 0.000 (0.202)	Prec@1 18.750 (22.480)	Prec@5 46.875 (57.964)	Loss 4.5009 (4.4429)   loss_c 2.3581	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0043	
Train: [7][40/45], lr: 0.01143	Time 0.797 (0.287)	Data 0.687 (0.199)	Prec@1 21.875 (23.018)	Prec@5 53.125 (57.317)	Loss 4.5175 (4.4442)   loss_c 2.3594	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0023	
Train: [8][0/45], lr: 0.01134	Time 3.053 (3.053)	Data 2.898 (2.898)	Prec@1 21.875 (21.875)	Prec@5 53.125 (53.125)	Loss 4.5376 (4.5376)   loss_c 2.4527	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9989	
Train: [8][10/45], lr: 0.01117	Time 0.058 (0.451)	Data 0.000 (0.362)	Prec@1 25.000 (23.580)	Prec@5 62.500 (59.943)	Loss 4.3711 (4.4268)   loss_c 2.3420	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9966	
Train: [8][20/45], lr: 0.01101	Time 0.073 (0.333)	Data 0.000 (0.248)	Prec@1 21.875 (22.619)	Prec@5 53.125 (57.887)	Loss 4.4509 (4.4441)   loss_c 2.3593	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9885	
Train: [8][30/45], lr: 0.01085	Time 0.076 (0.295)	Data 0.000 (0.209)	Prec@1 34.375 (23.790)	Prec@5 43.750 (56.754)	Loss 4.3618 (4.4366)   loss_c 2.3518	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9929	
Train: [8][40/45], lr: 0.01070	Time 0.761 (0.288)	Data 0.657 (0.203)	Prec@1 21.875 (23.628)	Prec@5 62.500 (57.851)	Loss 4.4200 (4.4329)   loss_c 2.3482	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9931	
Train: [9][0/45], lr: 0.01062	Time 3.089 (3.089)	Data 2.932 (2.932)	Prec@1 31.250 (31.250)	Prec@5 59.375 (59.375)	Loss 4.3423 (4.3423)   loss_c 2.2576	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9915	
Train: [9][10/45], lr: 0.01048	Time 0.116 (0.457)	Data 0.000 (0.357)	Prec@1 34.375 (22.159)	Prec@5 84.375 (58.523)	Loss 4.2613 (4.4538)   loss_c 2.3691	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9948	
Train: [9][20/45], lr: 0.01034	Time 0.072 (0.331)	Data 0.000 (0.238)	Prec@1 25.000 (23.214)	Prec@5 59.375 (57.738)	Loss 4.4331 (4.4444)   loss_c 2.3596	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9957	
Train: [9][30/45], lr: 0.01020	Time 0.080 (0.288)	Data 0.000 (0.197)	Prec@1 21.875 (22.782)	Prec@5 56.250 (58.065)	Loss 4.4724 (4.4490)   loss_c 2.3643	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9940	
Train: [9][40/45], lr: 0.01007	Time 0.992 (0.288)	Data 0.844 (0.198)	Prec@1 18.750 (23.018)	Prec@5 62.500 (58.079)	Loss 4.4341 (4.4454)   loss_c 2.3606	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9947	
Train: [10][0/45], lr: 0.01000	Time 3.257 (3.257)	Data 2.987 (2.987)	Prec@1 25.000 (25.000)	Prec@5 65.625 (65.625)	Loss 4.3610 (4.3610)   loss_c 2.2763	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9885	
Train: [10][10/45], lr: 0.00987	Time 0.118 (0.461)	Data 0.000 (0.354)	Prec@1 25.000 (21.307)	Prec@5 62.500 (56.534)	Loss 4.4526 (4.4696)   loss_c 2.3849	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9939	
Train: [10][20/45], lr: 0.00975	Time 0.064 (0.329)	Data 0.000 (0.233)	Prec@1 31.250 (24.107)	Prec@5 65.625 (58.929)	Loss 4.3525 (4.4343)   loss_c 2.3495	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9946	
Train: [10][30/45], lr: 0.00963	Time 0.077 (0.289)	Data 0.000 (0.198)	Prec@1 12.500 (23.992)	Prec@5 43.750 (57.964)	Loss 4.6014 (4.4378)   loss_c 2.3530	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9897	
Train: [10][40/45], lr: 0.00952	Time 1.046 (0.294)	Data 0.934 (0.205)	Prec@1 15.625 (23.247)	Prec@5 43.750 (57.774)	Loss 4.5666 (4.4413)   loss_c 2.3565	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9859	
Train: [11][0/45], lr: 0.00946	Time 3.085 (3.085)	Data 2.910 (2.910)	Prec@1 37.500 (37.500)	Prec@5 62.500 (62.500)	Loss 4.2669 (4.2669)   loss_c 2.1822	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9828	
Train: [11][10/45], lr: 0.00935	Time 0.064 (0.453)	Data 0.000 (0.355)	Prec@1 28.125 (25.852)	Prec@5 65.625 (61.932)	Loss 4.3677 (4.4056)   loss_c 2.3209	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9727	
Train: [11][20/45], lr: 0.00924	Time 0.081 (0.325)	Data 0.000 (0.236)	Prec@1 12.500 (24.256)	Prec@5 40.625 (57.143)	Loss 4.5599 (4.4264)   loss_c 2.3417	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9704	
Train: [11][30/45], lr: 0.00913	Time 0.066 (0.283)	Data 0.000 (0.196)	Prec@1 18.750 (23.589)	Prec@5 59.375 (58.468)	Loss 4.4854 (4.4320)   loss_c 2.3473	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9717	
Train: [11][40/45], lr: 0.00903	Time 0.661 (0.280)	Data 0.562 (0.194)	Prec@1 12.500 (22.485)	Prec@5 59.375 (57.851)	Loss 4.5515 (4.4445)   loss_c 2.3598	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9731	
Train: [12][0/45], lr: 0.00898	Time 3.026 (3.026)	Data 2.852 (2.852)	Prec@1 18.750 (18.750)	Prec@5 50.000 (50.000)	Loss 4.4871 (4.4871)   loss_c 2.4023	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9999	
Train: [12][10/45], lr: 0.00888	Time 0.119 (0.446)	Data 0.000 (0.350)	Prec@1 21.875 (23.011)	Prec@5 53.125 (56.250)	Loss 4.4159 (4.4388)   loss_c 2.3541	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9866	
Train: [12][20/45], lr: 0.00879	Time 0.070 (0.329)	Data 0.000 (0.237)	Prec@1 15.625 (21.726)	Prec@5 62.500 (56.399)	Loss 4.5038 (4.4596)   loss_c 2.3749	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9854	
Train: [12][30/45], lr: 0.00869	Time 0.066 (0.290)	Data 0.000 (0.200)	Prec@1 21.875 (22.480)	Prec@5 59.375 (58.165)	Loss 4.5109 (4.4450)   loss_c 2.3603	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9865	
Train: [12][40/45], lr: 0.00860	Time 0.833 (0.287)	Data 0.720 (0.199)	Prec@1 34.375 (22.866)	Prec@5 59.375 (58.537)	Loss 4.3629 (4.4401)   loss_c 2.3553	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9858	
Train: [13][0/45], lr: 0.00856	Time 2.995 (2.995)	Data 2.834 (2.834)	Prec@1 43.750 (43.750)	Prec@5 65.625 (65.625)	Loss 4.2711 (4.2711)   loss_c 2.1864	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9766	
Train: [13][10/45], lr: 0.00847	Time 0.114 (0.446)	Data 0.000 (0.343)	Prec@1 25.000 (23.864)	Prec@5 65.625 (58.239)	Loss 4.3356 (4.4305)   loss_c 2.3458	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9764	
Train: [13][20/45], lr: 0.00838	Time 0.065 (0.325)	Data 0.000 (0.226)	Prec@1 18.750 (22.619)	Prec@5 59.375 (57.440)	Loss 4.4942 (4.4470)   loss_c 2.3623	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9767	
Train: [13][30/45], lr: 0.00830	Time 0.070 (0.285)	Data 0.000 (0.190)	Prec@1 21.875 (22.379)	Prec@5 43.750 (56.956)	Loss 4.4977 (4.4477)   loss_c 2.3630	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9782	
Train: [13][40/45], lr: 0.00822	Time 0.870 (0.285)	Data 0.768 (0.191)	Prec@1 28.125 (23.018)	Prec@5 53.125 (57.241)	Loss 4.4801 (4.4412)   loss_c 2.3565	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9788	
Train: [14][0/45], lr: 0.00818	Time 3.101 (3.101)	Data 2.957 (2.957)	Prec@1 25.000 (25.000)	Prec@5 75.000 (75.000)	Loss 4.3526 (4.3526)   loss_c 2.2680	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9668	
Train: [14][10/45], lr: 0.00810	Time 0.111 (0.455)	Data 0.001 (0.359)	Prec@1 25.000 (24.716)	Prec@5 59.375 (61.932)	Loss 4.4385 (4.4223)   loss_c 2.3376	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9729	
Train: [14][20/45], lr: 0.00802	Time 0.067 (0.330)	Data 0.000 (0.237)	Prec@1 15.625 (22.619)	Prec@5 62.500 (59.821)	Loss 4.5547 (4.4468)   loss_c 2.3621	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9737	
Train: [14][30/45], lr: 0.00794	Time 0.078 (0.293)	Data 0.000 (0.201)	Prec@1 25.000 (22.581)	Prec@5 62.500 (58.367)	Loss 4.4202 (4.4491)   loss_c 2.3644	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9756	
Train: [14][40/45], lr: 0.00787	Time 0.754 (0.286)	Data 0.659 (0.197)	Prec@1 18.750 (22.942)	Prec@5 43.750 (57.927)	Loss 4.5410 (4.4427)   loss_c 2.3580	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9776	
Train: [15][0/45], lr: 0.00783	Time 3.044 (3.044)	Data 2.869 (2.869)	Prec@1 25.000 (25.000)	Prec@5 59.375 (59.375)	Loss 4.4375 (4.4375)   loss_c 2.3528	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9838	
Train: [15][10/45], lr: 0.00776	Time 0.134 (0.453)	Data 0.001 (0.350)	Prec@1 25.000 (25.568)	Prec@5 62.500 (58.239)	Loss 4.3275 (4.4160)   loss_c 2.3313	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9796	
Train: [15][20/45], lr: 0.00769	Time 0.077 (0.324)	Data 0.000 (0.230)	Prec@1 21.875 (24.702)	Prec@5 62.500 (58.333)	Loss 4.3893 (4.4169)   loss_c 2.3322	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9782	
Train: [15][30/45], lr: 0.00762	Time 0.070 (0.288)	Data 0.000 (0.195)	Prec@1 15.625 (22.681)	Prec@5 46.875 (56.653)	Loss 4.5577 (4.4496)   loss_c 2.3649	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9772	
Train: [15][40/45], lr: 0.00755	Time 0.852 (0.285)	Data 0.761 (0.193)	Prec@1 12.500 (22.485)	Prec@5 56.250 (57.622)	Loss 4.5664 (4.4442)   loss_c 2.3595	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9795	
Train: [16][0/45], lr: 0.00752	Time 3.152 (3.152)	Data 3.031 (3.031)	Prec@1 15.625 (15.625)	Prec@5 65.625 (65.625)	Loss 4.4684 (4.4684)   loss_c 2.3836	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9888	
Train: [16][10/45], lr: 0.00746	Time 0.078 (0.463)	Data 0.000 (0.376)	Prec@1 18.750 (21.591)	Prec@5 40.625 (59.091)	Loss 4.5690 (4.4503)   loss_c 2.3656	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9866	
Train: [16][20/45], lr: 0.00739	Time 0.062 (0.334)	Data 0.000 (0.247)	Prec@1 21.875 (23.810)	Prec@5 65.625 (58.631)	Loss 4.4670 (4.4290)   loss_c 2.3443	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9860	
Train: [16][30/45], lr: 0.00733	Time 0.066 (0.288)	Data 0.000 (0.200)	Prec@1 21.875 (23.891)	Prec@5 56.250 (58.770)	Loss 4.4620 (4.4309)   loss_c 2.3462	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9832	
Train: [16][40/45], lr: 0.00727	Time 0.825 (0.283)	Data 0.732 (0.197)	Prec@1 9.375 (22.713)	Prec@5 53.125 (57.927)	Loss 4.6193 (4.4437)   loss_c 2.3590	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9813	
Train: [17][0/45], lr: 0.00724	Time 3.045 (3.045)	Data 2.916 (2.916)	Prec@1 25.000 (25.000)	Prec@5 50.000 (50.000)	Loss 4.4467 (4.4467)   loss_c 2.3620	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9746	
Train: [17][10/45], lr: 0.00718	Time 0.124 (0.458)	Data 0.000 (0.357)	Prec@1 28.125 (25.284)	Prec@5 65.625 (63.068)	Loss 4.3830 (4.4042)   loss_c 2.3195	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9780	
Train: [17][20/45], lr: 0.00712	Time 0.052 (0.324)	Data 0.000 (0.235)	Prec@1 18.750 (23.363)	Prec@5 34.375 (59.375)	Loss 4.5835 (4.4450)   loss_c 2.3603	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9779	
Train: [17][30/45], lr: 0.00706	Time 0.065 (0.287)	Data 0.000 (0.197)	Prec@1 21.875 (22.984)	Prec@5 59.375 (59.375)	Loss 4.4233 (4.4441)   loss_c 2.3594	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9790	
Train: [17][40/45], lr: 0.00700	Time 0.842 (0.287)	Data 0.762 (0.196)	Prec@1 12.500 (23.095)	Prec@5 31.250 (58.918)	Loss 4.6194 (4.4414)   loss_c 2.3566	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9800	
Train: [18][0/45], lr: 0.00698	Time 3.039 (3.039)	Data 2.874 (2.874)	Prec@1 21.875 (21.875)	Prec@5 65.625 (65.625)	Loss 4.3915 (4.3915)   loss_c 2.3068	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9840	
Train: [18][10/45], lr: 0.00692	Time 0.140 (0.454)	Data 0.001 (0.353)	Prec@1 25.000 (25.284)	Prec@5 59.375 (56.818)	Loss 4.4601 (4.4270)   loss_c 2.3423	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9814	
Train: [18][20/45], lr: 0.00687	Time 0.065 (0.327)	Data 0.000 (0.233)	Prec@1 21.875 (22.917)	Prec@5 53.125 (56.548)	Loss 4.5164 (4.4520)   loss_c 2.3673	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9806	
Train: [18][30/45], lr: 0.00681	Time 0.057 (0.292)	Data 0.000 (0.200)	Prec@1 25.000 (22.681)	Prec@5 46.875 (57.157)	Loss 4.4583 (4.4495)   loss_c 2.3648	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9820	
Train: [18][40/45], lr: 0.00676	Time 0.736 (0.284)	Data 0.675 (0.194)	Prec@1 21.875 (22.713)	Prec@5 46.875 (57.927)	Loss 4.4640 (4.4459)   loss_c 2.3612	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9831	
Train: [19][0/45], lr: 0.00674	Time 3.019 (3.019)	Data 2.876 (2.876)	Prec@1 21.875 (21.875)	Prec@5 62.500 (62.500)	Loss 4.4668 (4.4668)   loss_c 2.3821	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9813	
Train: [19][10/45], lr: 0.00669	Time 0.127 (0.451)	Data 0.001 (0.353)	Prec@1 25.000 (25.000)	Prec@5 59.375 (59.943)	Loss 4.4143 (4.4192)   loss_c 2.3345	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9819	
Train: [19][20/45], lr: 0.00664	Time 0.066 (0.324)	Data 0.000 (0.229)	Prec@1 34.375 (24.107)	Prec@5 56.250 (57.292)	Loss 4.3118 (4.4261)   loss_c 2.3414	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9807	
Train: [19][30/45], lr: 0.00659	Time 0.066 (0.286)	Data 0.000 (0.193)	Prec@1 12.500 (23.185)	Prec@5 43.750 (56.653)	Loss 4.5632 (4.4410)   loss_c 2.3563	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9810	
Train: [19][40/45], lr: 0.00654	Time 0.797 (0.284)	Data 0.695 (0.194)	Prec@1 18.750 (23.399)	Prec@5 50.000 (57.393)	Loss 4.4897 (4.4351)   loss_c 2.3504	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9802	
Train: [20][0/45], lr: 0.00652	Time 2.974 (2.974)	Data 2.866 (2.866)	Prec@1 18.750 (18.750)	Prec@5 65.625 (65.625)	Loss 4.5012 (4.5012)   loss_c 2.4165	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9792	
Train: [20][10/45], lr: 0.00647	Time 0.070 (0.449)	Data 0.001 (0.367)	Prec@1 31.250 (23.864)	Prec@5 53.125 (59.659)	Loss 4.3466 (4.4213)   loss_c 2.3366	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9788	
Train: [20][20/45], lr: 0.00642	Time 0.069 (0.324)	Data 0.000 (0.244)	Prec@1 34.375 (23.512)	Prec@5 56.250 (58.780)	Loss 4.3356 (4.4275)   loss_c 2.3428	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9799	
Train: [20][30/45], lr: 0.00638	Time 0.066 (0.284)	Data 0.000 (0.204)	Prec@1 21.875 (22.984)	Prec@5 53.125 (57.762)	Loss 4.4777 (4.4446)   loss_c 2.3599	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9807	
Train: [20][40/45], lr: 0.00633	Time 0.976 (0.287)	Data 0.840 (0.205)	Prec@1 43.750 (23.247)	Prec@5 81.250 (58.232)	Loss 4.1305 (4.4391)   loss_c 2.3544	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9812	
Train: [21][0/45], lr: 0.00631	Time 3.068 (3.068)	Data 2.915 (2.915)	Prec@1 18.750 (18.750)	Prec@5 50.000 (50.000)	Loss 4.4655 (4.4655)   loss_c 2.3807	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9792	
Train: [21][10/45], lr: 0.00627	Time 0.116 (0.455)	Data 0.001 (0.360)	Prec@1 15.625 (21.591)	Prec@5 59.375 (59.091)	Loss 4.4913 (4.4560)   loss_c 2.3712	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9807	
Train: [21][20/45], lr: 0.00622	Time 0.053 (0.326)	Data 0.000 (0.239)	Prec@1 15.625 (21.726)	Prec@5 53.125 (57.440)	Loss 4.5272 (4.4558)   loss_c 2.3711	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9814	
Train: [21][30/45], lr: 0.00618	Time 0.068 (0.282)	Data 0.000 (0.196)	Prec@1 6.250 (22.782)	Prec@5 34.375 (58.266)	Loss 4.7314 (4.4440)   loss_c 2.3593	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9820	
Train: [21][40/45], lr: 0.00614	Time 0.975 (0.287)	Data 0.855 (0.201)	Prec@1 37.500 (23.247)	Prec@5 59.375 (58.079)	Loss 4.3207 (4.4403)   loss_c 2.3556	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9817	
Train: [22][0/45], lr: 0.00612	Time 3.097 (3.097)	Data 2.974 (2.974)	Prec@1 31.250 (31.250)	Prec@5 62.500 (62.500)	Loss 4.3499 (4.3499)   loss_c 2.2652	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9796	
Train: [22][10/45], lr: 0.00608	Time 0.060 (0.454)	Data 0.000 (0.354)	Prec@1 12.500 (19.886)	Prec@5 50.000 (53.977)	Loss 4.5590 (4.4903)   loss_c 2.4056	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9806	
Train: [22][20/45], lr: 0.00604	Time 0.072 (0.327)	Data 0.000 (0.236)	Prec@1 40.625 (21.429)	Prec@5 71.875 (58.185)	Loss 4.2120 (4.4528)   loss_c 2.3680	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9833	
Train: [22][30/45], lr: 0.00600	Time 0.066 (0.282)	Data 0.000 (0.195)	Prec@1 15.625 (22.077)	Prec@5 56.250 (57.359)	Loss 4.4643 (4.4491)   loss_c 2.3644	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9844	
Train: [22][40/45], lr: 0.00596	Time 0.941 (0.283)	Data 0.872 (0.195)	Prec@1 28.125 (22.713)	Prec@5 62.500 (58.003)	Loss 4.3875 (4.4462)   loss_c 2.3614	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9843	
Train: [23][0/45], lr: 0.00594	Time 3.065 (3.065)	Data 2.910 (2.910)	Prec@1 18.750 (18.750)	Prec@5 59.375 (59.375)	Loss 4.4973 (4.4973)   loss_c 2.4126	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9839	
Train: [23][10/45], lr: 0.00591	Time 0.061 (0.457)	Data 0.001 (0.365)	Prec@1 31.250 (24.432)	Prec@5 43.750 (55.966)	Loss 4.4217 (4.4262)   loss_c 2.3415	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9790	
Train: [23][20/45], lr: 0.00587	Time 0.070 (0.329)	Data 0.000 (0.241)	Prec@1 21.875 (24.702)	Prec@5 53.125 (56.548)	Loss 4.5222 (4.4247)   loss_c 2.3400	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9772	
Train: [23][30/45], lr: 0.00583	Time 0.064 (0.284)	Data 0.000 (0.196)	Prec@1 12.500 (23.589)	Prec@5 50.000 (56.956)	Loss 4.5837 (4.4363)   loss_c 2.3516	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9752	
Train: [23][40/45], lr: 0.00579	Time 0.952 (0.287)	Data 0.830 (0.198)	Prec@1 28.125 (23.018)	Prec@5 71.875 (58.079)	Loss 4.2750 (4.4385)   loss_c 2.3538	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9744	
Train: [24][0/45], lr: 0.00578	Time 3.031 (3.031)	Data 2.882 (2.882)	Prec@1 15.625 (15.625)	Prec@5 53.125 (53.125)	Loss 4.5369 (4.5369)   loss_c 2.4522	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9733	
Train: [24][10/45], lr: 0.00574	Time 0.057 (0.455)	Data 0.000 (0.356)	Prec@1 37.500 (25.000)	Prec@5 78.125 (59.943)	Loss 4.2355 (4.4055)   loss_c 2.3208	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9770	
Train: [24][20/45], lr: 0.00571	Time 0.067 (0.326)	Data 0.000 (0.233)	Prec@1 21.875 (25.446)	Prec@5 68.750 (59.375)	Loss 4.4532 (4.4155)   loss_c 2.3308	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9755	
Train: [24][30/45], lr: 0.00567	Time 0.072 (0.287)	Data 0.000 (0.196)	Prec@1 28.125 (23.790)	Prec@5 46.875 (58.468)	Loss 4.4081 (4.4323)   loss_c 2.3476	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9744	
Train: [24][40/45], lr: 0.00564	Time 0.640 (0.280)	Data 0.540 (0.190)	Prec@1 25.000 (23.171)	Prec@5 53.125 (58.003)	Loss 4.4310 (4.4388)   loss_c 2.3541	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9748	
Train: [25][0/45], lr: 0.00562	Time 3.076 (3.076)	Data 2.938 (2.938)	Prec@1 28.125 (28.125)	Prec@5 62.500 (62.500)	Loss 4.3730 (4.3730)   loss_c 2.2883	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9773	
Train: [25][10/45], lr: 0.00559	Time 0.061 (0.458)	Data 0.000 (0.362)	Prec@1 31.250 (24.148)	Prec@5 56.250 (55.966)	Loss 4.4015 (4.4280)   loss_c 2.3433	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9807	
Train: [25][20/45], lr: 0.00556	Time 0.068 (0.326)	Data 0.000 (0.239)	Prec@1 18.750 (23.214)	Prec@5 43.750 (57.738)	Loss 4.5596 (4.4394)   loss_c 2.3546	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9792	
Train: [25][30/45], lr: 0.00552	Time 0.056 (0.284)	Data 0.000 (0.199)	Prec@1 15.625 (23.790)	Prec@5 59.375 (58.065)	Loss 4.5056 (4.4323)   loss_c 2.3476	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9777	
Train: [25][40/45], lr: 0.00549	Time 1.689 (0.306)	Data 1.596 (0.221)	Prec@1 28.125 (22.866)	Prec@5 50.000 (58.308)	Loss 4.4463 (4.4387)   loss_c 2.3540	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9777	
Train: [26][0/45], lr: 0.00548	Time 3.121 (3.121)	Data 2.916 (2.916)	Prec@1 18.750 (18.750)	Prec@5 62.500 (62.500)	Loss 4.4537 (4.4537)   loss_c 2.3690	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9790	
Train: [26][10/45], lr: 0.00544	Time 0.101 (0.460)	Data 0.000 (0.350)	Prec@1 15.625 (22.159)	Prec@5 53.125 (56.534)	Loss 4.5167 (4.4531)   loss_c 2.3684	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9804	
Train: [26][20/45], lr: 0.00541	Time 0.069 (0.329)	Data 0.000 (0.229)	Prec@1 21.875 (22.768)	Prec@5 62.500 (58.185)	Loss 4.4541 (4.4410)   loss_c 2.3563	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9804	
Train: [26][30/45], lr: 0.00538	Time 0.067 (0.287)	Data 0.000 (0.187)	Prec@1 28.125 (24.093)	Prec@5 71.875 (58.569)	Loss 4.3485 (4.4316)   loss_c 2.3469	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9802	
Train: [26][40/45], lr: 0.00535	Time 0.809 (0.284)	Data 0.685 (0.187)	Prec@1 12.500 (23.476)	Prec@5 46.875 (58.460)	Loss 4.6236 (4.4364)   loss_c 2.3517	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9796	
Train: [27][0/45], lr: 0.00534	Time 3.089 (3.089)	Data 2.954 (2.954)	Prec@1 18.750 (18.750)	Prec@5 53.125 (53.125)	Loss 4.4837 (4.4837)   loss_c 2.3990	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9816	
Train: [27][10/45], lr: 0.00531	Time 0.121 (0.455)	Data 0.001 (0.357)	Prec@1 18.750 (22.159)	Prec@5 53.125 (60.227)	Loss 4.4659 (4.4442)   loss_c 2.3595	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9798	
Train: [27][20/45], lr: 0.00528	Time 0.071 (0.327)	Data 0.000 (0.235)	Prec@1 37.500 (23.065)	Prec@5 68.750 (60.119)	Loss 4.2665 (4.4282)   loss_c 2.3435	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9807	
Train: [27][30/45], lr: 0.00525	Time 0.065 (0.285)	Data 0.000 (0.193)	Prec@1 21.875 (23.488)	Prec@5 56.250 (57.762)	Loss 4.4505 (4.4379)   loss_c 2.3532	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9807	
Train: [27][40/45], lr: 0.00522	Time 0.935 (0.286)	Data 0.836 (0.195)	Prec@1 12.500 (22.713)	Prec@5 37.500 (57.088)	Loss 4.6005 (4.4459)   loss_c 2.3612	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9806	
Train: [28][0/45], lr: 0.00521	Time 3.000 (3.000)	Data 2.869 (2.869)	Prec@1 25.000 (25.000)	Prec@5 59.375 (59.375)	Loss 4.4390 (4.4390)   loss_c 2.3543	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9820	
Train: [28][10/45], lr: 0.00518	Time 0.130 (0.453)	Data 0.000 (0.350)	Prec@1 12.500 (21.307)	Prec@5 50.000 (59.375)	Loss 4.5577 (4.4389)   loss_c 2.3542	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9818	
Train: [28][20/45], lr: 0.00515	Time 0.082 (0.329)	Data 0.000 (0.229)	Prec@1 18.750 (22.470)	Prec@5 59.375 (59.077)	Loss 4.5009 (4.4397)   loss_c 2.3550	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9806	
Train: [28][30/45], lr: 0.00513	Time 0.063 (0.286)	Data 0.000 (0.189)	Prec@1 18.750 (22.681)	Prec@5 56.250 (58.065)	Loss 4.4650 (4.4457)   loss_c 2.3610	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9807	
Train: [28][40/45], lr: 0.00510	Time 0.784 (0.284)	Data 0.699 (0.189)	Prec@1 21.875 (22.713)	Prec@5 62.500 (57.774)	Loss 4.4165 (4.4474)   loss_c 2.3627	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9810	
Train: [29][0/45], lr: 0.00509	Time 3.131 (3.131)	Data 2.926 (2.926)	Prec@1 18.750 (18.750)	Prec@5 53.125 (53.125)	Loss 4.4933 (4.4933)   loss_c 2.4086	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9803	
Train: [29][10/45], lr: 0.00506	Time 0.064 (0.458)	Data 0.000 (0.350)	Prec@1 37.500 (25.568)	Prec@5 62.500 (59.659)	Loss 4.3009 (4.4196)   loss_c 2.3349	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9802	
Train: [29][20/45], lr: 0.00503	Time 0.068 (0.325)	Data 0.000 (0.229)	Prec@1 15.625 (22.917)	Prec@5 43.750 (58.780)	Loss 4.5527 (4.4441)   loss_c 2.3594	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9795	
Train: [29][30/45], lr: 0.00501	Time 0.061 (0.285)	Data 0.000 (0.192)	Prec@1 15.625 (22.379)	Prec@5 56.250 (58.165)	Loss 4.4885 (4.4495)   loss_c 2.3648	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9802	
Train: [29][40/45], lr: 0.00498	Time 0.921 (0.286)	Data 0.818 (0.196)	Prec@1 18.750 (22.637)	Prec@5 56.250 (57.698)	Loss 4.4210 (4.4437)   loss_c 2.3589	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9808	
Train: [30][0/45], lr: 0.00497	Time 3.139 (3.139)	Data 2.963 (2.963)	Prec@1 34.375 (34.375)	Prec@5 71.875 (71.875)	Loss 4.2650 (4.2650)   loss_c 2.1803	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9863	
Train: [30][10/45], lr: 0.00494	Time 0.079 (0.465)	Data 0.001 (0.363)	Prec@1 28.125 (23.580)	Prec@5 71.875 (60.511)	Loss 4.3922 (4.4271)   loss_c 2.3424	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9812	
Train: [30][20/45], lr: 0.00492	Time 0.071 (0.333)	Data 0.000 (0.240)	Prec@1 34.375 (23.214)	Prec@5 50.000 (58.929)	Loss 4.3781 (4.4351)   loss_c 2.3504	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9804	
Train: [30][30/45], lr: 0.00490	Time 0.065 (0.287)	Data 0.000 (0.197)	Prec@1 31.250 (23.286)	Prec@5 68.750 (58.871)	Loss 4.3017 (4.4314)   loss_c 2.3467	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9795	
Train: [30][40/45], lr: 0.00487	Time 0.995 (0.290)	Data 0.899 (0.201)	Prec@1 9.375 (22.866)	Prec@5 62.500 (58.003)	Loss 4.4836 (4.4405)   loss_c 2.3558	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9793	
total time: 536.747 