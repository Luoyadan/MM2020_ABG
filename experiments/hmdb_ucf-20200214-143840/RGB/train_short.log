Train: [1][40/45], lr: 0.02087	Time 0.077 (0.340)	Data 0.000 (0.132)	Prec@1 12.500 (19.665)	Prec@5 43.750 (51.601)	Loss 4.6187 (4.8244)   loss_c 2.7386	beta 0.750, 0.750, 0.500  loss_a 2.0746	gamma 0.003000  loss_e 3.7414	
Train: [2][40/45], lr: 0.01814	Time 1.560 (0.307)	Data 1.462 (0.220)	Prec@1 12.500 (22.332)	Prec@5 46.875 (57.774)	Loss 4.6517 (4.4938)   loss_c 2.4091	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9643	
Train: [3][40/45], lr: 0.01612	Time 0.905 (0.286)	Data 0.792 (0.197)	Prec@1 18.750 (22.485)	Prec@5 65.625 (57.165)	Loss 4.5127 (4.4618)   loss_c 2.3770	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0086	
Train: [4][40/45], lr: 0.01456	Time 0.985 (0.287)	Data 0.878 (0.201)	Prec@1 21.875 (23.171)	Prec@5 71.875 (57.470)	Loss 4.4728 (4.4567)   loss_c 2.3719	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0153	
Train: [5][40/45], lr: 0.01331	Time 1.234 (0.292)	Data 1.129 (0.203)	Prec@1 18.750 (23.171)	Prec@5 37.500 (57.774)	Loss 4.5607 (4.4418)   loss_c 2.3570	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0084	
Train: [6][40/45], lr: 0.01228	Time 1.152 (0.294)	Data 1.061 (0.205)	Prec@1 25.000 (23.171)	Prec@5 56.250 (56.555)	Loss 4.4183 (4.4446)   loss_c 2.3598	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0120	
Train: [7][40/45], lr: 0.01143	Time 0.797 (0.287)	Data 0.687 (0.199)	Prec@1 21.875 (23.018)	Prec@5 53.125 (57.317)	Loss 4.5175 (4.4442)   loss_c 2.3594	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 4.0023	
Train: [8][40/45], lr: 0.01070	Time 0.761 (0.288)	Data 0.657 (0.203)	Prec@1 21.875 (23.628)	Prec@5 62.500 (57.851)	Loss 4.4200 (4.4329)   loss_c 2.3482	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9931	
Train: [9][40/45], lr: 0.01007	Time 0.992 (0.288)	Data 0.844 (0.198)	Prec@1 18.750 (23.018)	Prec@5 62.500 (58.079)	Loss 4.4341 (4.4454)   loss_c 2.3606	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9947	
Train: [10][40/45], lr: 0.00952	Time 1.046 (0.294)	Data 0.934 (0.205)	Prec@1 15.625 (23.247)	Prec@5 43.750 (57.774)	Loss 4.5666 (4.4413)   loss_c 2.3565	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9859	
Train: [11][40/45], lr: 0.00903	Time 0.661 (0.280)	Data 0.562 (0.194)	Prec@1 12.500 (22.485)	Prec@5 59.375 (57.851)	Loss 4.5515 (4.4445)   loss_c 2.3598	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9731	
Train: [12][40/45], lr: 0.00860	Time 0.833 (0.287)	Data 0.720 (0.199)	Prec@1 34.375 (22.866)	Prec@5 59.375 (58.537)	Loss 4.3629 (4.4401)   loss_c 2.3553	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9858	
Train: [13][40/45], lr: 0.00822	Time 0.870 (0.285)	Data 0.768 (0.191)	Prec@1 28.125 (23.018)	Prec@5 53.125 (57.241)	Loss 4.4801 (4.4412)   loss_c 2.3565	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9788	
Train: [14][40/45], lr: 0.00787	Time 0.754 (0.286)	Data 0.659 (0.197)	Prec@1 18.750 (22.942)	Prec@5 43.750 (57.927)	Loss 4.5410 (4.4427)   loss_c 2.3580	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9776	
Train: [15][40/45], lr: 0.00755	Time 0.852 (0.285)	Data 0.761 (0.193)	Prec@1 12.500 (22.485)	Prec@5 56.250 (57.622)	Loss 4.5664 (4.4442)   loss_c 2.3595	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9795	
Train: [16][40/45], lr: 0.00727	Time 0.825 (0.283)	Data 0.732 (0.197)	Prec@1 9.375 (22.713)	Prec@5 53.125 (57.927)	Loss 4.6193 (4.4437)   loss_c 2.3590	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9813	
Train: [17][40/45], lr: 0.00700	Time 0.842 (0.287)	Data 0.762 (0.196)	Prec@1 12.500 (23.095)	Prec@5 31.250 (58.918)	Loss 4.6194 (4.4414)   loss_c 2.3566	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9800	
Train: [18][40/45], lr: 0.00676	Time 0.736 (0.284)	Data 0.675 (0.194)	Prec@1 21.875 (22.713)	Prec@5 46.875 (57.927)	Loss 4.4640 (4.4459)   loss_c 2.3612	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9831	
Train: [19][40/45], lr: 0.00654	Time 0.797 (0.284)	Data 0.695 (0.194)	Prec@1 18.750 (23.399)	Prec@5 50.000 (57.393)	Loss 4.4897 (4.4351)   loss_c 2.3504	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9802	
Train: [20][40/45], lr: 0.00633	Time 0.976 (0.287)	Data 0.840 (0.205)	Prec@1 43.750 (23.247)	Prec@5 81.250 (58.232)	Loss 4.1305 (4.4391)   loss_c 2.3544	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9812	
Train: [21][40/45], lr: 0.00614	Time 0.975 (0.287)	Data 0.855 (0.201)	Prec@1 37.500 (23.247)	Prec@5 59.375 (58.079)	Loss 4.3207 (4.4403)   loss_c 2.3556	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9817	
Train: [22][40/45], lr: 0.00596	Time 0.941 (0.283)	Data 0.872 (0.195)	Prec@1 28.125 (22.713)	Prec@5 62.500 (58.003)	Loss 4.3875 (4.4462)   loss_c 2.3614	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9843	
Train: [23][40/45], lr: 0.00579	Time 0.952 (0.287)	Data 0.830 (0.198)	Prec@1 28.125 (23.018)	Prec@5 71.875 (58.079)	Loss 4.2750 (4.4385)   loss_c 2.3538	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9744	
Train: [24][40/45], lr: 0.00564	Time 0.640 (0.280)	Data 0.540 (0.190)	Prec@1 25.000 (23.171)	Prec@5 53.125 (58.003)	Loss 4.4310 (4.4388)   loss_c 2.3541	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9748	
Train: [25][40/45], lr: 0.00549	Time 1.689 (0.306)	Data 1.596 (0.221)	Prec@1 28.125 (22.866)	Prec@5 50.000 (58.308)	Loss 4.4463 (4.4387)   loss_c 2.3540	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9777	
Train: [26][40/45], lr: 0.00535	Time 0.809 (0.284)	Data 0.685 (0.187)	Prec@1 12.500 (23.476)	Prec@5 46.875 (58.460)	Loss 4.6236 (4.4364)   loss_c 2.3517	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9796	
Train: [27][40/45], lr: 0.00522	Time 0.935 (0.286)	Data 0.836 (0.195)	Prec@1 12.500 (22.713)	Prec@5 37.500 (57.088)	Loss 4.6005 (4.4459)   loss_c 2.3612	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9806	
Train: [28][40/45], lr: 0.00510	Time 0.784 (0.284)	Data 0.699 (0.189)	Prec@1 21.875 (22.713)	Prec@5 62.500 (57.774)	Loss 4.4165 (4.4474)   loss_c 2.3627	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9810	
Train: [29][40/45], lr: 0.00498	Time 0.921 (0.286)	Data 0.818 (0.196)	Prec@1 18.750 (22.637)	Prec@5 56.250 (57.698)	Loss 4.4210 (4.4437)   loss_c 2.3589	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9808	
Train: [30][40/45], lr: 0.00487	Time 0.995 (0.290)	Data 0.899 (0.201)	Prec@1 9.375 (22.866)	Prec@5 62.500 (58.003)	Loss 4.4836 (4.4405)   loss_c 2.3558	beta 0.750, 0.750, 0.500  loss_a 2.0728	gamma 0.003000  loss_e 3.9793	
total time: 536.747 